{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice 2. ConvLab을 이용한 Pipelined 대화 시스템 구축\n",
    "\n",
    "# 목차\n",
    "\n",
    "## Step 1. MultiWOZ 데이터셋을 살펴보자\n",
    "\n",
    "## Step 2. ConvLab을 활용해 Pipelined 대화 시스템을 구축하자\n",
    "\n",
    "## Step 3. ConvLab에서 제공하는 모듈들로 모델들을 구성 및 진단하고, 평가하자\n",
    "\n",
    "## Additional. End-to-end Neural Pipeline (ACL 2020) 모델을 사용해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. MultiWOZ 데이터셋을 살펴보자\n",
    "\n",
    "## Step 1.0 필요한 module들을 정의합니다\n",
    "\n",
    "아래 코드를 도와주는 module들 이며, 필요시 수정할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import zipfile\n",
    "import sys\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from textwrap import indent\n",
    "from pprint import pformat\n",
    "from pprint import pprint\n",
    "\n",
    "def read_zipped_json(filepath, filename):\n",
    "    archive = zipfile.ZipFile(filepath, 'r')\n",
    "    return json.load(archive.open(filename))\n",
    "\n",
    "\n",
    "def pprint_manual(user_manual, name):\n",
    "    \"\"\"print user manual\n",
    "        argument 'name' is needed to discriminate 'WOZ' from others\n",
    "    \"\"\"\n",
    "    print('    User manual (message) : ')\n",
    "    if 'WOZ' in name:\n",
    "        print(\" \"*8, user_manual)\n",
    "    else:\n",
    "        for manual_one in user_manual:\n",
    "            print(\" \"*8, manual_one)\n",
    "\n",
    "\n",
    "def pprint_goal(goal, name):\n",
    "    \"\"\"print user's goal\n",
    "        argument 'name' is needed to discriminate \"WOZ\" from others.\n",
    "    \"\"\"\n",
    "    if 'WOZ' in name:\n",
    "        pass\n",
    "    else:\n",
    "        for i, mes in enumerate(goal['message']):\n",
    "            mes = mes.replace('<span class=\\'emphasis\\'>', '')\n",
    "            mes = mes.replace('</span>', '')\n",
    "            goal['message'][i] = mes\n",
    "\n",
    "    print(\"[Goals]\")\n",
    "    user_manual = None\n",
    "    for key, value in goal.items():\n",
    "        if not value:           # empty\n",
    "            continue\n",
    "        elif key == 'message':  # user manual\n",
    "            user_manual = value\n",
    "        else:                   # valid domain\n",
    "            domain = key        \n",
    "            print(indent(pformat({domain : value}), ' '*4))\n",
    "    pprint_manual(user_manual, name)\n",
    "    \n",
    "\n",
    "def get_valid_domains(goal):\n",
    "    \"\"\"return valid domains for pretty print\"\"\"\n",
    "    domains = []\n",
    "    for key, value in goal.items():\n",
    "        if not value:           # empty\n",
    "            continue\n",
    "        elif key == 'message':  # user manual\n",
    "            continue\n",
    "        else:                   # valid domain\n",
    "            domains.append(key)\n",
    "    return domains\n",
    "\n",
    "\n",
    "def pprint_turns(log, domains):\n",
    "    \"\"\"pretty print for dialogue turns\"\"\"\n",
    "    \n",
    "    # signal for stopping print\n",
    "    signal = None\n",
    "    \n",
    "    for i, log_one in enumerate(log):\n",
    "        \n",
    "        # dummy input function for pausing\n",
    "        print('-' * 20 + '1. Enter to keep going 2. Type \\'stop\\' and Enter to stop printing ' + '-' * 40)\n",
    "        signal = input()\n",
    "        if 'stop' in signal:\n",
    "            break\n",
    "\n",
    "        # check whether system turn or not\n",
    "        bool_sys_turn = False\n",
    "        if log_one['metadata']:\n",
    "            bool_sys_turn = True\n",
    "\n",
    "        # delete span_info\n",
    "        if 'span_info' in log_one:\n",
    "            del log_one['span_info']\n",
    "\n",
    "        # delete unnecessary domains\n",
    "        domain_pairs = log_one['metadata']\n",
    "        del_domains = []\n",
    "        for dom, _ in domain_pairs.items():\n",
    "            if not dom in domains:\n",
    "                del_domains.append(dom)\n",
    "        for dom in del_domains:\n",
    "            del domain_pairs[dom]\n",
    "    \n",
    "        # pretty print\n",
    "        if bool_sys_turn: print(\"[SYS]\", end=\" \")\n",
    "        else:             print(\"[USR]\", end=\" \")\n",
    "        print(\"(turn {})\".format(i))\n",
    "\n",
    "        log_one['1. dialogue_state'] = log_one['metadata']\n",
    "        log_one['2. dialogue_act'] = log_one['dialog_act']\n",
    "        log_one['3. text'] = log_one['text']\n",
    "        del log_one['metadata']\n",
    "        del log_one['dialog_act']\n",
    "        del log_one['text']\n",
    "        print(indent(pformat(log_one, width=100), ' ' * 4))\n",
    "    \n",
    "    # transform signal to boolean\n",
    "    if 'stop' in signal:\n",
    "        signal = True\n",
    "    else: \n",
    "        signal = False\n",
    "    return signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.1 MultiWOZ 데이터셋을 불러옵니다.\n",
    "\n",
    "MultiWOZ 데이터셋은 7개의 domain ('hotel', 'train', 'attraction', 'restaurant' 'taxi', 'policy', 'hospital') 으로 구성되어 있으며, 여행정보를 얻고자 하는 'user' 와 이를 도와주는 'system'이 나누는 대화에 대한 데이터 셋입니다.\n",
    "약 10,000개의 대화 뭉치로 구성되어 있으며, train, validation, test용으로 구분되어 있습니다.\n",
    "\n",
    "아래 코드를 실행하면, MultiWOZ 데이터 셋 내 train용 데이터 이름 100개가 출력 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current directory : /home/donghoon/PycharmProjects/samsung_dialogue_tutorial\n",
      "load ConvLab-2/data/multiwoz/train.json...! \n",
      "number of dialogues : 8434\n",
      "load ConvLab-2/data/multiwoz/val.json...! \n",
      "number of dialogues : 999\n",
      "load ConvLab-2/data/multiwoz/test.json...! \n",
      "number of dialogues : 1000\n",
      "\n",
      "['SNG01856', 'SNG0129', 'MUL2168', 'SNG01445', 'MUL2105', 'PMUL1690', 'MUL2395', 'SNG0190', 'PMUL1170', 'SNG01741', 'PMUL4899', 'MUL2261', 'SSNG0348', 'MUL0784', 'MUL0886', 'PMUL2512', 'SNG0548', 'MUL1474', 'PMUL4372', 'PMUL4047', 'PMUL0151', 'MUL0586', 'PMUL3552', 'PMUL1539', 'MUL1790', 'PMUL3021', 'SNG0699', 'SNG0228', 'PMUL3296', 'MUL1434', 'PMUL2203', 'PMUL3250', 'PMUL0510', 'MUL1124', 'PMUL3719', 'SNG0297', 'PMUL2049', 'SNG01722', 'PMUL2100', 'MUL1853', 'MUL2694', 'SNG1006', 'SNG1345', 'MUL1299', 'MUL1490', 'PMUL2749', 'MUL1628', 'PMUL2202', 'SNG01450', 'SNG0131', 'SNG0984', 'PMUL1419', 'SNG0798', 'MUL0161', 'PMUL2803', 'MUL0925', 'MUL1005', 'SNG0104', 'SNG1197', 'MUL1265', 'WOZ20259', 'MUL1223', 'PMUL2596', 'MUL2037', 'MUL1497', 'MUL2256', 'MUL0076', 'WOZ20114', 'MUL1100', 'PMUL4469', 'PMUL0263', 'PMUL3597', 'PMUL1454', 'MUL1618', 'PMUL3342', 'PMUL0906', 'PMUL0695', 'PMUL0307', 'PMUL3045', 'MUL1113', 'SNG0385', 'SNG0285', 'SNG1134', 'PMUL2082', 'PMUL3031', 'PMUL1499', 'SNG1097', 'PMUL0608', 'PMUL4191', 'MUL0265', 'PMUL0433', 'WOZ20214', 'MUL1567', 'WOZ20593', 'PMUL4487', 'MUL2535', 'PMUL0552', 'PMUL0621', 'PMUL1229', 'PMUL1169']\n"
     ]
    }
   ],
   "source": [
    "cur_dir = os.path.abspath(os.curdir)\n",
    "print(\"current directory :\", cur_dir)\n",
    "data_dir = \"ConvLab-2/data/multiwoz\"\n",
    "processed_data_dir = os.path.join(cur_dir, 'multiwoz_data/all_data')\n",
    "if not os.path.exists(processed_data_dir):\n",
    "    os.makedirs(processed_data_dir)\n",
    "\n",
    "data_key = ['train', 'val', 'test']\n",
    "data = {}\n",
    "for key in data_key:\n",
    "    data[key] = read_zipped_json(os.path.join(data_dir, key + '.json.zip'), key + '.json')\n",
    "    print('load {}.json...! '.format(os.path.join(data_dir, key)))\n",
    "    print('number of dialogues : {}'.format(len(data[key])))\n",
    "print()\n",
    "\n",
    "# print available dialogue name until 100\n",
    "print(list(data['train'].keys())[:100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.2 데이터가 어떻게 생겼는지 살펴봅시다.\n",
    "\n",
    "위의 출력된 데이터 이름들 중 몇개를 파이썬 리스트 안에 삽입하여 dialogue 를 확인할 수 있습니다. (ex. names = \\['SNG0943', 'MUL1801'] ))\n",
    "\n",
    "한 dialogue는 크게\n",
    "\n",
    "1. user의 goal, (코드 상에서 'goal'))\n",
    "\n",
    "2. system과 user의 대화, (코드 상에서 'dialogue_turns'))\n",
    "\n",
    "로 구분 됩니다.\n",
    "\n",
    "***\n",
    "\n",
    "user (\\[USR]])는 정의된 goal 및 manual을 읽고, 해당 goal을 달성하기 위해 대화를 수행합니다.\n",
    "\n",
    "system (\\[SYS])은 user의 goal을 알지 못하고, 대화를 통해 (1)user가 원하는 조건을 파악하고, (2)user가 원하는 정보를 제공하며, (3)필요시 예약을 수행합니다.\n",
    "\n",
    "***\n",
    "\n",
    "goal 내에서,\n",
    "\n",
    "`info`는 user 입장에서, system에게 user가 원하는 조건 및 니즈를 알려주고자(inform) 하는 내용이고,\n",
    "\n",
    "`reqt`는 user 입장에서, system에게 uesr가 요청하고자(request) 하는 내용입니다.\n",
    "\n",
    "***\n",
    "\n",
    "본 데이터셋의 경우 __system model을 만드는 것__을 목표로 합니다.\n",
    "\n",
    "***\n",
    "\n",
    "아래 코드를 실행하면 goal 및 발화를 살펴볼 수 있으며, Enter로 넘길 수 있습니다. \n",
    "\n",
    "그만 보고 싶다면 stop을 입력 후 Enter 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "[Dialogue name] 'SNG0943'\n",
      "[Goals]\n",
      "    {'hotel': {'fail_info': {},\n",
      "               'info': {'internet': 'yes',\n",
      "                        'parking': 'yes',\n",
      "                        'stars': '4',\n",
      "                        'type': 'guesthouse'},\n",
      "               'reqt': ['address', 'pricerange']}}\n",
      "    User manual (message) : \n",
      "         You are looking for a place to stay. The hotel should have a star of 4 and should include free parking\n",
      "         The hotel should be in the type of guesthouse and should include free wifi\n",
      "         Make sure you get address and price range\n",
      "--------------------1. Enter to keep going 2. Type 'stop' and Enter to stop printing ----------------------------------------\n",
      "\n",
      "[USR] (turn 0)\n",
      "    {'1. dialogue_state': {},\n",
      "     '2. dialogue_act': {'Hotel-Inform': [['Parking', 'yes'], ['Stars', '4']]},\n",
      "     '3. text': \"Howdy ! I 'm in town for the night and need a place to stay . I 'd like a four star \"\n",
      "                'joint with free parking , if you please .'}\n",
      "--------------------1. Enter to keep going 2. Type 'stop' and Enter to stop printing ----------------------------------------\n",
      "stop\n"
     ]
    }
   ],
   "source": [
    "# You can handle dialogue_names\n",
    "dialogue_names = ['SNG0943', 'MUL1801']\n",
    "\n",
    "for name in dialogue_names:\n",
    "    \n",
    "    print()\n",
    "    print('-' * 125)\n",
    "    print(\"[Dialogue name] \\'{}\\'\".format(name))\n",
    "\n",
    "    # access datum using name\n",
    "    datum = data['train'][name]\n",
    "    goal = datum['goal']\n",
    "    dialogue_turns = datum['log']\n",
    "\n",
    "    # print goal and dialogue turns\n",
    "    pprint_goal(goal, name)\n",
    "    valid_domains = get_valid_domains(goal)\n",
    "    break_signal = pprint_turns(dialogue_turns, valid_domains)  # If you don't want to see print, please comment!\n",
    "    # break_signal = pprint_turns(dialogue_turns, valid_domains)    Like this!\n",
    "\n",
    "    if break_signal:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. ConvLab을 활용해 Pipelined 대화 시스템을 구축하자\n",
    "\n",
    "## Step 2.0 필요한 module들을 정의합니다\n",
    "\n",
    "아래 코드를 도와주는 module들 이며, 필요시 수정할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0810 03:56:16.787091 139710458230592 file_utils.py:35] PyTorch version 1.1.0 available.\n",
      "I0810 03:56:19.440644 139710458230592 modeling_bert.py:226] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "I0810 03:56:19.444469 139710458230592 modeling_xlnet.py:339] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "I0810 03:56:19.608417 139710458230592 registrable.py:73] instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
      "I0810 03:56:19.610358 139710458230592 registrable.py:73] instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
      "I0810 03:56:19.611890 139710458230592 registrable.py:73] instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
      "I0810 03:56:19.613340 139710458230592 registrable.py:73] instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# common import: convlab2.$module.$model.$dataset\n",
    "from convlab2.nlu.jointBERT.multiwoz import BERTNLU\n",
    "from convlab2.nlu.milu.multiwoz import MILU\n",
    "from convlab2.dst.rule.multiwoz import RuleDST\n",
    "from convlab2.policy.rule.multiwoz import RulePolicy\n",
    "from convlab2.nlg.template.multiwoz import TemplateNLG\n",
    "from convlab2.dialog_agent import BiSession, Agent # , # PipelineAgent\n",
    "from convlab2.evaluator.multiwoz_eval import MultiWozEvaluator\n",
    "from pprint import pprint\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import spacy\n",
    "\n",
    "import logging \n",
    "# uncessary logging block\n",
    "mpl_logger = logging.getLogger('matplotlib') \n",
    "mpl_logger.setLevel(logging.WARNING) \n",
    "cntp_logger = logging.getLogger('urllib3.connectionpool')\n",
    "cntp_logger.setLevel(logging.WARNING)\n",
    "ttu_logger = logging.getLogger('transformers.tokenization_utils')\n",
    "ttu_logger.setLevel(logging.WARNING)\n",
    "tcu_logger = logging.getLogger('transformers.configuration_utils')\n",
    "tcu_logger.setLevel(logging.WARNING)\n",
    "tmu_logger = logging.getLogger('transformers.modeling_utils')\n",
    "tmu_logger.setLevel(logging.WARNING)\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "spacy.cli.download('en_core_web_sm')\n",
    "spacy.load('en_core_web_sm')\n",
    "\n",
    "from convlab2.nlu import NLU\n",
    "from convlab2.dst import DST\n",
    "from convlab2.policy import Policy\n",
    "from convlab2.nlg import NLG\n",
    "from copy import deepcopy\n",
    "\n",
    "class PipelineAgent(Agent):\n",
    "    \"\"\"Pipeline dialog agent base class, including NLU, DST, Policy and NLG.\n",
    "\n",
    "    The combination modes of pipeline agent modules are flexible. The only thing you have to make sure is that\n",
    "    the API of agents are matched.\n",
    "\n",
    "    Example:\n",
    "        If agent A is (nlu, tracker, policy), then the agent B should be like (tracker, policy, nlg) to ensure API\n",
    "        matching.\n",
    "    The valid module combinations are as follows:\n",
    "           =====   =====    ======  ===     ==      ===\n",
    "            NLU     DST     Policy  NLG     In      Out\n",
    "           =====   =====    ======  ===     ==      ===\n",
    "            \\+      \\+        \\+    \\+      nl      nl\n",
    "             o      \\+        \\+    \\+      da      nl\n",
    "             o      \\+        \\+     o      da      da\n",
    "            \\+      \\+        \\+     o      nl      da\n",
    "             o       o        \\+     o      da      da\n",
    "           =====   =====    ======  ===     ==      ===\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nlu: NLU, dst: DST, policy: Policy, nlg: NLG, name: str):\n",
    "        \"\"\"The constructor of PipelineAgent class.\n",
    "\n",
    "        Here are some special combination cases:\n",
    "\n",
    "            1. If you use word-level DST (such as Neural Belief Tracker), you should set the nlu_model paramater \\\n",
    "             to None. The agent will combine the modules automitically.\n",
    "\n",
    "            2. If you want to aggregate DST and Policy as a single module, set tracker to None.\n",
    "\n",
    "        Args:\n",
    "            nlu (NLU):\n",
    "                The natural langauge understanding module of agent.\n",
    "\n",
    "            dst (DST):\n",
    "                The dialog state tracker of agent.\n",
    "\n",
    "            policy (Policy):\n",
    "                The dialog policy module of agent.\n",
    "\n",
    "            nlg (NLG):\n",
    "                The natural langauge generator module of agent.\n",
    "        \"\"\"\n",
    "        super(PipelineAgent, self).__init__(name=name)\n",
    "        assert self.name in ['user', 'sys']\n",
    "        self.opponent_name = 'user' if self.name is 'sys' else 'sys'\n",
    "        self.nlu = nlu\n",
    "        self.dst = dst\n",
    "        self.policy = policy\n",
    "        self.nlg = nlg\n",
    "        self.init_session()\n",
    "        self.history = []\n",
    "\n",
    "        self.print_nlu = False\n",
    "        self.print_dst = False\n",
    "        self.print_pol = False\n",
    "\n",
    "    def response(self, observation, print_nlu=False, print_dst=False, print_pol=False):\n",
    "        \"\"\"Generate agent response using the agent modules.\"\"\"\n",
    "        # Note: If you modify the logic of this function, please ensure that it is consistent with deploy.server.ServerCtrl._turn()\n",
    "\n",
    "        self.print_nlu = print_nlu\n",
    "        self.print_dst = print_dst\n",
    "        self.print_pol = print_pol\n",
    "\n",
    "        if self.dst is not None:\n",
    "            self.dst.state['history'].append([self.opponent_name, observation]) # [['sys', sys_utt], ['user', user_utt],...]\n",
    "        self.history.append([self.opponent_name, observation])\n",
    "        # get dialog act\n",
    "        if self.nlu is not None:\n",
    "            self.input_action = self.nlu.predict(observation, context=[x[1] for x in self.history[:-1]])\n",
    "        else:\n",
    "            self.input_action = observation\n",
    "        self.input_action = deepcopy(self.input_action) # get rid of reference problem\n",
    "        if self.print_nlu:\n",
    "            print(\"nlu predict\")\n",
    "            pprint(self.input_action)\n",
    "        # get state\n",
    "        if self.dst is not None:\n",
    "            if self.name is 'sys':\n",
    "                self.dst.state['user_action'] = self.input_action\n",
    "            else:\n",
    "                self.dst.state['system_action'] = self.input_action\n",
    "            state = self.dst.update(self.input_action)\n",
    "        else:\n",
    "            state = self.input_action\n",
    "        state = deepcopy(state) # get rid of reference problem\n",
    "        if self.print_dst:\n",
    "            print(\"dialogue state predict\")\n",
    "            pprint({'dialogue state': state['belief_state'], 'history': state['history']})\n",
    "        # get action\n",
    "        self.output_action = deepcopy(self.policy.predict(state)) # get rid of reference problem\n",
    "        if self.print_pol:\n",
    "            print(\"dialogue act predict\")\n",
    "            pprint(self.output_action)\n",
    "\n",
    "        # get model response\n",
    "        if self.nlg is not None:\n",
    "            model_response = self.nlg.generate(self.output_action)\n",
    "        else:\n",
    "            model_response = self.output_action\n",
    "        # print(\"model response {}\".format(model_response))\n",
    "        if self.dst is not None:\n",
    "            self.dst.state['history'].append([self.name, model_response])\n",
    "            if self.name is 'sys':\n",
    "                self.dst.state['system_action'] = self.output_action\n",
    "            else:\n",
    "                self.dst.state['user_action'] = self.output_action\n",
    "        self.history.append([self.name, model_response])\n",
    "        return model_response\n",
    "\n",
    "    def is_terminated(self):\n",
    "        if hasattr(self.policy, 'is_terminated'):\n",
    "            return self.policy.is_terminated()\n",
    "        return None\n",
    "\n",
    "    def get_reward(self):\n",
    "        if hasattr(self.policy, 'get_reward'):\n",
    "            return self.policy.get_reward()\n",
    "        return None\n",
    "\n",
    "    def init_session(self):\n",
    "        \"\"\"Init the attributes of DST and Policy module.\"\"\"\n",
    "        if self.nlu is not None:\n",
    "            self.nlu.init_session()\n",
    "        if self.dst is not None:\n",
    "            self.dst.init_session()\n",
    "            if self.name == 'sys':\n",
    "                self.dst.state['history'].append([self.name, 'null'])\n",
    "        if self.policy is not None:\n",
    "            self.policy.init_session()\n",
    "        if self.nlg is not None:\n",
    "            self.nlg.init_session()\n",
    "        self.history = []\n",
    "\n",
    "    def get_in_da(self):\n",
    "        return self.input_action\n",
    "\n",
    "    def get_out_da(self):\n",
    "        return self.output_action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.1 pipelined 대화 시스템 예시를 살펴봅시다.\n",
    "\n",
    "우선, system model을 pipelined 대화 시스템으로 구성했을 때, 대화를 어떻게 수행할 수 있는지 살펴봅시다.\n",
    "\n",
    "pipelined 대화 모델은 크게 4가지로 구성되어 있습니다.\n",
    "\n",
    "NLU (Natural Language Understanding (언어 인식 모듈)) : 직전의 상대방 발화를 이해 및 해석 합니다.\n",
    "\n",
    "DST (Dialogue State Tracking (대화 상태 추적 모듈)) : 현재까지 대화의 맥락을 추적하고, 변경 사항을 업데이트 합니다.\n",
    "\n",
    "Dialogue Policy (대화 정책 모듈) : 다음 발화를 위해 구조화된 단어 형태로 정책을 결정 합니다. (자연스러운 문장의 형태가 아닌, 의도만을 결정합니다.)\n",
    "\n",
    "NLG (Natural Language Generation (언어 생성 모듈)) : 결정된 정책을 가지고, 사람이 이해할 수 있는 자연어를 생성합니다.\n",
    "\n",
    "-----------------\n",
    "\n",
    "아래는 가장 기본적인 Pipelined 대화 시스템을 구성한 예시입니다.\n",
    "\n",
    "BERT NLU : 앞선 practice 1에서 다룬 BERT NLU\n",
    "\n",
    "RuleDST : Rule 기반 DST module\n",
    "\n",
    "RulePolicy : Rule 기반 Policy module\n",
    "\n",
    "TemplateNLG : Template 기반 (정해진 템플릿 위에서 단어를 채워넣는 방식) NLG module\n",
    "\n",
    "4가지 모듈에 대해 정의를 끝마쳤다면, PipelineAgent라는 wrapper 에 씌워 sys_agent를 선언합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load from https://convlab.blob.core.windows.net/convlab-2/milu_multiwoz_all_context.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0810 03:56:31.223443 139710458230592 archival.py:173] loading archive file /home/donghoon/.convlab2/cache/adeae6e5151198de3b1634fcb630a0243cd2e098a75703260c4489c6e92a5f51.4fff038b1c36dbb489d93575f604cf72276b651ac6b370b5a48f8f67df2ed971\n",
      "I0810 03:56:31.224578 139710458230592 archival.py:182] extracting archive file /home/donghoon/.convlab2/cache/adeae6e5151198de3b1634fcb630a0243cd2e098a75703260c4489c6e92a5f51.4fff038b1c36dbb489d93575f604cf72276b651ac6b370b5a48f8f67df2ed971 to temp dir /tmp/tmpelk1y8yc\n",
      "I0810 03:56:31.346318 139710458230592 registrable.py:73] instantiating registered subclass milu of <class 'allennlp.models.model.Model'>\n",
      "I0810 03:56:31.347058 139710458230592 params.py:265] type = default\n",
      "I0810 03:56:31.347500 139710458230592 registrable.py:73] instantiating registered subclass default of <class 'allennlp.data.vocabulary.Vocabulary'>\n",
      "I0810 03:56:31.347939 139710458230592 vocabulary.py:306] Loading token dictionary from /tmp/tmpelk1y8yc/vocabulary.\n",
      "I0810 03:56:31.364222 139710458230592 from_params.py:340] instantiating class <class 'allennlp.models.model.Model'> from params {'attention': {'matrix_dim': 400, 'type': 'bilinear', 'vector_dim': 400}, 'attention_for_intent': False, 'attention_for_tag': False, 'context_for_intent': True, 'context_for_tag': False, 'dropout': 0.3, 'encoder': {'bidirectional': True, 'dropout': 0.5, 'hidden_size': 200, 'input_size': 178, 'num_layers': 1, 'type': 'lstm'}, 'include_start_end_transitions': False, 'intent_encoder': {'bidirectional': True, 'dropout': 0.5, 'hidden_size': 200, 'input_size': 400, 'num_layers': 1, 'type': 'lstm'}, 'label_encoding': 'BIO', 'regularizer': [['scalar_parameters', {'alpha': 0.1, 'type': 'l2'}]], 'text_field_embedder': {'token_embedders': {'token_characters': {'embedding': {'embedding_dim': 16}, 'encoder': {'conv_layer_activation': 'relu', 'embedding_dim': 16, 'ngram_filter_sizes': [3], 'num_filters': 128, 'type': 'cnn'}, 'type': 'character_encoding'}, 'tokens': {'embedding_dim': 50, 'trainable': True, 'type': 'embedding'}}}, 'type': 'milu'} and extras {'vocab'}\n",
      "I0810 03:56:31.364998 139710458230592 params.py:265] model.type = milu\n",
      "I0810 03:56:31.365544 139710458230592 from_params.py:340] instantiating class <class 'convlab2.nlu.milu.model.MILU'> from params {'attention': {'matrix_dim': 400, 'type': 'bilinear', 'vector_dim': 400}, 'attention_for_intent': False, 'attention_for_tag': False, 'context_for_intent': True, 'context_for_tag': False, 'dropout': 0.3, 'encoder': {'bidirectional': True, 'dropout': 0.5, 'hidden_size': 200, 'input_size': 178, 'num_layers': 1, 'type': 'lstm'}, 'include_start_end_transitions': False, 'intent_encoder': {'bidirectional': True, 'dropout': 0.5, 'hidden_size': 200, 'input_size': 400, 'num_layers': 1, 'type': 'lstm'}, 'label_encoding': 'BIO', 'regularizer': [['scalar_parameters', {'alpha': 0.1, 'type': 'l2'}]], 'text_field_embedder': {'token_embedders': {'token_characters': {'embedding': {'embedding_dim': 16}, 'encoder': {'conv_layer_activation': 'relu', 'embedding_dim': 16, 'ngram_filter_sizes': [3], 'num_filters': 128, 'type': 'cnn'}, 'type': 'character_encoding'}, 'tokens': {'embedding_dim': 50, 'trainable': True, 'type': 'embedding'}}}} and extras {'vocab'}\n",
      "I0810 03:56:31.366178 139710458230592 from_params.py:340] instantiating class <class 'allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder'> from params {'token_embedders': {'token_characters': {'embedding': {'embedding_dim': 16}, 'encoder': {'conv_layer_activation': 'relu', 'embedding_dim': 16, 'ngram_filter_sizes': [3], 'num_filters': 128, 'type': 'cnn'}, 'type': 'character_encoding'}, 'tokens': {'embedding_dim': 50, 'trainable': True, 'type': 'embedding'}}} and extras {'vocab'}\n",
      "I0810 03:56:31.366560 139710458230592 params.py:265] model.text_field_embedder.type = basic\n",
      "I0810 03:56:31.367009 139710458230592 params.py:265] model.text_field_embedder.embedder_to_indexer_map = None\n",
      "I0810 03:56:31.367417 139710458230592 params.py:265] model.text_field_embedder.allow_unmatched_keys = False\n",
      "I0810 03:56:31.367831 139710458230592 from_params.py:340] instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'embedding': {'embedding_dim': 16}, 'encoder': {'conv_layer_activation': 'relu', 'embedding_dim': 16, 'ngram_filter_sizes': [3], 'num_filters': 128, 'type': 'cnn'}, 'type': 'character_encoding'} and extras {'vocab'}\n",
      "I0810 03:56:31.368211 139710458230592 params.py:265] model.text_field_embedder.token_embedders.token_characters.type = character_encoding\n",
      "I0810 03:56:31.368707 139710458230592 params.py:265] model.text_field_embedder.token_embedders.token_characters.embedding.num_embeddings = None\n",
      "I0810 03:56:31.369115 139710458230592 params.py:265] model.text_field_embedder.token_embedders.token_characters.embedding.vocab_namespace = token_characters\n",
      "I0810 03:56:31.369482 139710458230592 params.py:265] model.text_field_embedder.token_embedders.token_characters.embedding.embedding_dim = 16\n",
      "I0810 03:56:31.369845 139710458230592 params.py:265] model.text_field_embedder.token_embedders.token_characters.embedding.pretrained_file = None\n",
      "I0810 03:56:31.370205 139710458230592 params.py:265] model.text_field_embedder.token_embedders.token_characters.embedding.projection_dim = None\n",
      "I0810 03:56:31.370561 139710458230592 params.py:265] model.text_field_embedder.token_embedders.token_characters.embedding.trainable = True\n",
      "I0810 03:56:31.370912 139710458230592 params.py:265] model.text_field_embedder.token_embedders.token_characters.embedding.padding_index = None\n",
      "I0810 03:56:31.371324 139710458230592 params.py:265] model.text_field_embedder.token_embedders.token_characters.embedding.max_norm = None\n",
      "I0810 03:56:31.371749 139710458230592 params.py:265] model.text_field_embedder.token_embedders.token_characters.embedding.norm_type = 2.0\n",
      "I0810 03:56:31.372139 139710458230592 params.py:265] model.text_field_embedder.token_embedders.token_characters.embedding.scale_grad_by_freq = False\n",
      "I0810 03:56:31.372489 139710458230592 params.py:265] model.text_field_embedder.token_embedders.token_characters.embedding.sparse = False\n",
      "I0810 03:56:31.375281 139710458230592 from_params.py:340] instantiating class <class 'allennlp.modules.seq2vec_encoders.seq2vec_encoder.Seq2VecEncoder'> from params {'conv_layer_activation': 'relu', 'embedding_dim': 16, 'ngram_filter_sizes': [3], 'num_filters': 128, 'type': 'cnn'} and extras set()\n",
      "I0810 03:56:31.375803 139710458230592 params.py:265] model.text_field_embedder.token_embedders.token_characters.encoder.type = cnn\n",
      "I0810 03:56:31.376370 139710458230592 from_params.py:340] instantiating class <class 'allennlp.modules.seq2vec_encoders.cnn_encoder.CnnEncoder'> from params {'conv_layer_activation': 'relu', 'embedding_dim': 16, 'ngram_filter_sizes': [3], 'num_filters': 128} and extras set()\n",
      "I0810 03:56:31.376960 139710458230592 params.py:265] model.text_field_embedder.token_embedders.token_characters.encoder.embedding_dim = 16\n",
      "I0810 03:56:31.377498 139710458230592 params.py:265] model.text_field_embedder.token_embedders.token_characters.encoder.num_filters = 128\n",
      "I0810 03:56:31.378043 139710458230592 params.py:265] model.text_field_embedder.token_embedders.token_characters.encoder.ngram_filter_sizes = [3]\n",
      "I0810 03:56:31.378556 139710458230592 params.py:265] model.text_field_embedder.token_embedders.token_characters.encoder.conv_layer_activation = relu\n",
      "I0810 03:56:31.379165 139710458230592 registrable.py:73] instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
      "I0810 03:56:31.379746 139710458230592 params.py:265] model.text_field_embedder.token_embedders.token_characters.encoder.output_dim = None\n",
      "I0810 03:56:31.384845 139710458230592 params.py:265] model.text_field_embedder.token_embedders.token_characters.dropout = 0.0\n",
      "I0810 03:56:31.385490 139710458230592 from_params.py:340] instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'embedding_dim': 50, 'trainable': True, 'type': 'embedding'} and extras {'vocab'}\n",
      "I0810 03:56:31.385970 139710458230592 params.py:265] model.text_field_embedder.token_embedders.tokens.type = embedding\n",
      "I0810 03:56:31.386822 139710458230592 params.py:265] model.text_field_embedder.token_embedders.tokens.num_embeddings = None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0810 03:56:31.387293 139710458230592 params.py:265] model.text_field_embedder.token_embedders.tokens.vocab_namespace = tokens\n",
      "I0810 03:56:31.387852 139710458230592 params.py:265] model.text_field_embedder.token_embedders.tokens.embedding_dim = 50\n",
      "I0810 03:56:31.388443 139710458230592 params.py:265] model.text_field_embedder.token_embedders.tokens.pretrained_file = None\n",
      "I0810 03:56:31.388859 139710458230592 params.py:265] model.text_field_embedder.token_embedders.tokens.projection_dim = None\n",
      "I0810 03:56:31.389339 139710458230592 params.py:265] model.text_field_embedder.token_embedders.tokens.trainable = True\n",
      "I0810 03:56:31.389777 139710458230592 params.py:265] model.text_field_embedder.token_embedders.tokens.padding_index = None\n",
      "I0810 03:56:31.391076 139710458230592 params.py:265] model.text_field_embedder.token_embedders.tokens.max_norm = None\n",
      "I0810 03:56:31.391502 139710458230592 params.py:265] model.text_field_embedder.token_embedders.tokens.norm_type = 2.0\n",
      "I0810 03:56:31.391897 139710458230592 params.py:265] model.text_field_embedder.token_embedders.tokens.scale_grad_by_freq = False\n",
      "I0810 03:56:31.392347 139710458230592 params.py:265] model.text_field_embedder.token_embedders.tokens.sparse = False\n",
      "I0810 03:56:31.399888 139710458230592 from_params.py:340] instantiating class <class 'allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder'> from params {'bidirectional': True, 'dropout': 0.5, 'hidden_size': 200, 'input_size': 178, 'num_layers': 1, 'type': 'lstm'} and extras {'vocab'}\n",
      "I0810 03:56:31.400380 139710458230592 params.py:265] model.encoder.type = lstm\n",
      "I0810 03:56:31.400873 139710458230592 params.py:265] model.encoder.batch_first = True\n",
      "I0810 03:56:31.401318 139710458230592 params.py:265] model.encoder.stateful = False\n",
      "I0810 03:56:31.401764 139710458230592 params.py:394] Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "I0810 03:56:31.402208 139710458230592 params.py:397] CURRENTLY DEFINED PARAMETERS: \n",
      "I0810 03:56:31.402651 139710458230592 params.py:392] model.encoder.bidirectional = True\n",
      "I0810 03:56:31.403844 139710458230592 params.py:392] model.encoder.dropout = 0.5\n",
      "I0810 03:56:31.404304 139710458230592 params.py:392] model.encoder.hidden_size = 200\n",
      "I0810 03:56:31.404778 139710458230592 params.py:392] model.encoder.input_size = 178\n",
      "I0810 03:56:31.405330 139710458230592 params.py:392] model.encoder.num_layers = 1\n",
      "I0810 03:56:31.405858 139710458230592 params.py:392] model.encoder.batch_first = True\n",
      "I0810 03:56:31.413784 139710458230592 from_params.py:340] instantiating class <class 'allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder'> from params {'bidirectional': True, 'dropout': 0.5, 'hidden_size': 200, 'input_size': 400, 'num_layers': 1, 'type': 'lstm'} and extras {'vocab'}\n",
      "I0810 03:56:31.414530 139710458230592 params.py:265] model.intent_encoder.type = lstm\n",
      "I0810 03:56:31.415189 139710458230592 params.py:265] model.intent_encoder.batch_first = True\n",
      "I0810 03:56:31.415956 139710458230592 params.py:265] model.intent_encoder.stateful = False\n",
      "I0810 03:56:31.416384 139710458230592 params.py:394] Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "I0810 03:56:31.416793 139710458230592 params.py:397] CURRENTLY DEFINED PARAMETERS: \n",
      "I0810 03:56:31.417202 139710458230592 params.py:392] model.intent_encoder.bidirectional = True\n",
      "I0810 03:56:31.417771 139710458230592 params.py:392] model.intent_encoder.dropout = 0.5\n",
      "I0810 03:56:31.418707 139710458230592 params.py:392] model.intent_encoder.hidden_size = 200\n",
      "I0810 03:56:31.419169 139710458230592 params.py:392] model.intent_encoder.input_size = 400\n",
      "I0810 03:56:31.419582 139710458230592 params.py:392] model.intent_encoder.num_layers = 1\n",
      "I0810 03:56:31.420021 139710458230592 params.py:392] model.intent_encoder.batch_first = True\n",
      "I0810 03:56:31.428292 139710458230592 from_params.py:340] instantiating class <class 'allennlp.modules.attention.attention.Attention'> from params {'matrix_dim': 400, 'type': 'bilinear', 'vector_dim': 400} and extras {'vocab'}\n",
      "I0810 03:56:31.428866 139710458230592 params.py:265] model.attention.type = bilinear\n",
      "I0810 03:56:31.429394 139710458230592 from_params.py:340] instantiating class <class 'allennlp.modules.attention.bilinear_attention.BilinearAttention'> from params {'matrix_dim': 400, 'vector_dim': 400} and extras {'vocab'}\n",
      "I0810 03:56:31.429891 139710458230592 params.py:265] model.attention.vector_dim = 400\n",
      "I0810 03:56:31.430339 139710458230592 params.py:265] model.attention.matrix_dim = 400\n",
      "I0810 03:56:31.431323 139710458230592 params.py:265] model.attention.normalize = True\n",
      "I0810 03:56:31.432112 139710458230592 registrable.py:73] instantiating registered subclass linear of <class 'allennlp.nn.activations.Activation'>\n",
      "I0810 03:56:31.433804 139710458230592 params.py:265] model.context_for_intent = True\n",
      "I0810 03:56:31.434225 139710458230592 params.py:265] model.context_for_tag = False\n",
      "I0810 03:56:31.434680 139710458230592 params.py:265] model.attention_for_intent = False\n",
      "I0810 03:56:31.435193 139710458230592 params.py:265] model.attention_for_tag = False\n",
      "I0810 03:56:31.435633 139710458230592 params.py:265] model.sequence_label_namespace = labels\n",
      "I0810 03:56:31.436536 139710458230592 params.py:265] model.intent_label_namespace = intent_labels\n",
      "I0810 03:56:31.436995 139710458230592 params.py:265] model.label_encoding = BIO\n",
      "I0810 03:56:31.437422 139710458230592 params.py:265] model.include_start_end_transitions = False\n",
      "I0810 03:56:31.437932 139710458230592 params.py:265] model.crf_decoding = False\n",
      "I0810 03:56:31.438452 139710458230592 params.py:265] model.constrain_crf_decoding = None\n",
      "I0810 03:56:31.438988 139710458230592 params.py:265] model.focal_loss_gamma = None\n",
      "I0810 03:56:31.439494 139710458230592 params.py:265] model.nongeneral_intent_weight = 5.0\n",
      "I0810 03:56:31.439930 139710458230592 params.py:265] model.num_train_examples = None\n",
      "I0810 03:56:31.440310 139710458230592 params.py:265] model.calculate_span_f1 = None\n",
      "I0810 03:56:31.441808 139710458230592 params.py:265] model.dropout = 0.3\n",
      "I0810 03:56:31.442182 139710458230592 params.py:265] model.verbose_metrics = False\n",
      "I0810 03:56:31.442684 139710458230592 params.py:265] model.regularizer.0.1.type = l2\n",
      "I0810 03:56:31.443046 139710458230592 registrable.py:73] instantiating registered subclass l2 of <class 'allennlp.nn.regularizers.regularizer.Regularizer'>\n",
      "I0810 03:56:31.524194 139710458230592 initializers.py:307] Initializing parameters\n",
      "I0810 03:56:31.525032 139710458230592 initializers.py:323] Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "I0810 03:56:31.525641 139710458230592 initializers.py:328]    attention._bias\n",
      "I0810 03:56:31.526226 139710458230592 initializers.py:328]    attention._weight_matrix\n",
      "I0810 03:56:31.526847 139710458230592 initializers.py:328]    encoder._module.bias_hh_l0\n",
      "I0810 03:56:31.527427 139710458230592 initializers.py:328]    encoder._module.bias_hh_l0_reverse\n",
      "I0810 03:56:31.528014 139710458230592 initializers.py:328]    encoder._module.bias_ih_l0\n",
      "I0810 03:56:31.528575 139710458230592 initializers.py:328]    encoder._module.bias_ih_l0_reverse\n",
      "I0810 03:56:31.530553 139710458230592 initializers.py:328]    encoder._module.weight_hh_l0\n",
      "I0810 03:56:31.531112 139710458230592 initializers.py:328]    encoder._module.weight_hh_l0_reverse\n",
      "I0810 03:56:31.531709 139710458230592 initializers.py:328]    encoder._module.weight_ih_l0\n",
      "I0810 03:56:31.532270 139710458230592 initializers.py:328]    encoder._module.weight_ih_l0_reverse\n",
      "I0810 03:56:31.532857 139710458230592 initializers.py:328]    intent_encoder._module.bias_hh_l0\n",
      "I0810 03:56:31.533424 139710458230592 initializers.py:328]    intent_encoder._module.bias_hh_l0_reverse\n",
      "I0810 03:56:31.534022 139710458230592 initializers.py:328]    intent_encoder._module.bias_ih_l0\n",
      "I0810 03:56:31.534552 139710458230592 initializers.py:328]    intent_encoder._module.bias_ih_l0_reverse\n",
      "I0810 03:56:31.535113 139710458230592 initializers.py:328]    intent_encoder._module.weight_hh_l0\n",
      "I0810 03:56:31.535626 139710458230592 initializers.py:328]    intent_encoder._module.weight_hh_l0_reverse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0810 03:56:31.536191 139710458230592 initializers.py:328]    intent_encoder._module.weight_ih_l0\n",
      "I0810 03:56:31.536708 139710458230592 initializers.py:328]    intent_encoder._module.weight_ih_l0_reverse\n",
      "I0810 03:56:31.537280 139710458230592 initializers.py:328]    intent_projection_layer.bias\n",
      "I0810 03:56:31.538460 139710458230592 initializers.py:328]    intent_projection_layer.weight\n",
      "I0810 03:56:31.539056 139710458230592 initializers.py:328]    tag_projection_layer._module.bias\n",
      "I0810 03:56:31.539602 139710458230592 initializers.py:328]    tag_projection_layer._module.weight\n",
      "I0810 03:56:31.540225 139710458230592 initializers.py:328]    text_field_embedder.token_embedder_token_characters._embedding._module.weight\n",
      "I0810 03:56:31.540777 139710458230592 initializers.py:328]    text_field_embedder.token_embedder_token_characters._encoder._module.conv_layer_0.bias\n",
      "I0810 03:56:31.541268 139710458230592 initializers.py:328]    text_field_embedder.token_embedder_token_characters._encoder._module.conv_layer_0.weight\n",
      "I0810 03:56:31.541939 139710458230592 initializers.py:328]    text_field_embedder.token_embedder_tokens.weight\n",
      "I0810 03:56:33.836355 139710458230592 from_params.py:340] instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'context_size': 3, 'token_indexers': {'token_characters': {'min_padding_length': 3, 'type': 'characters'}, 'tokens': {'lowercase_tokens': True, 'type': 'single_id'}}, 'type': 'milu'} and extras set()\n",
      "I0810 03:56:33.836998 139710458230592 params.py:265] dataset_reader.type = milu\n",
      "I0810 03:56:33.837506 139710458230592 from_params.py:340] instantiating class <class 'convlab2.nlu.milu.dataset_reader.MILUDatasetReader'> from params {'context_size': 3, 'token_indexers': {'token_characters': {'min_padding_length': 3, 'type': 'characters'}, 'tokens': {'lowercase_tokens': True, 'type': 'single_id'}}} and extras set()\n",
      "I0810 03:56:33.838019 139710458230592 params.py:265] dataset_reader.context_size = 3\n",
      "I0810 03:56:33.838634 139710458230592 params.py:265] dataset_reader.agent = None\n",
      "I0810 03:56:33.839334 139710458230592 params.py:265] dataset_reader.random_context_size = True\n",
      "I0810 03:56:33.839797 139710458230592 params.py:265] dataset_reader.token_delimiter = None\n",
      "I0810 03:56:33.840333 139710458230592 from_params.py:340] instantiating class allennlp.data.token_indexers.token_indexer.TokenIndexer from params {'min_padding_length': 3, 'type': 'characters'} and extras set()\n",
      "I0810 03:56:33.840728 139710458230592 params.py:265] dataset_reader.token_indexers.token_characters.type = characters\n",
      "I0810 03:56:33.841157 139710458230592 from_params.py:340] instantiating class allennlp.data.token_indexers.token_characters_indexer.TokenCharactersIndexer from params {'min_padding_length': 3} and extras set()\n",
      "I0810 03:56:33.841629 139710458230592 params.py:265] dataset_reader.token_indexers.token_characters.namespace = token_characters\n",
      "I0810 03:56:33.842042 139710458230592 params.py:265] dataset_reader.token_indexers.token_characters.start_tokens = None\n",
      "I0810 03:56:33.842404 139710458230592 params.py:265] dataset_reader.token_indexers.token_characters.end_tokens = None\n",
      "I0810 03:56:33.842781 139710458230592 params.py:265] dataset_reader.token_indexers.token_characters.min_padding_length = 3\n",
      "I0810 03:56:33.843141 139710458230592 params.py:265] dataset_reader.token_indexers.token_characters.token_min_padding_length = 0\n",
      "I0810 03:56:33.843570 139710458230592 from_params.py:340] instantiating class allennlp.data.token_indexers.token_indexer.TokenIndexer from params {'lowercase_tokens': True, 'type': 'single_id'} and extras set()\n",
      "I0810 03:56:33.843908 139710458230592 params.py:265] dataset_reader.token_indexers.tokens.type = single_id\n",
      "I0810 03:56:33.844334 139710458230592 from_params.py:340] instantiating class allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer from params {'lowercase_tokens': True} and extras set()\n",
      "I0810 03:56:33.844723 139710458230592 params.py:265] dataset_reader.token_indexers.tokens.namespace = tokens\n",
      "I0810 03:56:33.845100 139710458230592 params.py:265] dataset_reader.token_indexers.tokens.lowercase_tokens = True\n",
      "I0810 03:56:33.845475 139710458230592 params.py:265] dataset_reader.token_indexers.tokens.start_tokens = None\n",
      "I0810 03:56:33.845830 139710458230592 params.py:265] dataset_reader.token_indexers.tokens.end_tokens = None\n",
      "I0810 03:56:33.846176 139710458230592 params.py:265] dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
      "I0810 03:56:33.846555 139710458230592 params.py:265] dataset_reader.lazy = False\n"
     ]
    }
   ],
   "source": [
    "# MILU\n",
    "sys_nlu = MILU()\n",
    "# simple rule DST\n",
    "sys_dst = RuleDST()\n",
    "# rule policy\n",
    "sys_policy = RulePolicy()\n",
    "# template NLG\n",
    "sys_nlg = TemplateNLG(is_user=False)\n",
    "# assemble\n",
    "sys_agent = PipelineAgent(sys_nlu, sys_dst, sys_policy, sys_nlg, name='sys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sys_agent.response(\"user의 발화\", print_nlu=False, print_dst=False, print_pol=False) 를 하면, user의 발화에 대한 응답을 합니다.\n",
    "\n",
    "print_nlu, print_dst, print_pol을 True로 변경하면, 해당하는 value를 print 해볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Would lovell lodge work for you ? I have 3 options for you.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys_agent.init_session()\n",
    "sys_agent.response(\"I want to find a moderate hotel\", print_nlu=False, print_dst=False, print_pol=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It is a hotel .'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys_agent.response(\"Which type of hotel is it ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The address is 74 chesterton road .'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys_agent.response(\"OK , where is its address ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are welcome.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys_agent.response(\"Thank you !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The good luck chinese food takeaway matches your description . I have 3 options for you.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys_agent.response(\"Try to find me a Chinese restaurant in south area .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It is chinese food .'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys_agent.response(\"Which kind of food it provides ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Booking was successful . Reference number is : 00000003 .'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys_agent.response(\"Book a table for 5 , this Sunday .\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.2 system agent와 대화할 user simulator를 구성해봅시다.\n",
    "\n",
    "system model의 성능을 알아보기 위해서는 user 역할을 할 수 있는 user simulator가 필요합니다. \n",
    "\n",
    "사람이 매번 user의 역할을 하여 대화를 주고 받는 것은 많은 노동력을 필요로 하기 때문입니다. \n",
    "\n",
    "특히, Dialog Policy를 RL agent로 두었을 때, 다양한 대화 시도를 위해서 user simulator 는 필수적입니다.\n",
    "\n",
    "ConvLab에서는 RulePolicy(character='usr')로 두었을 때, `Agenda` policy로 변환되며, 이는 user의 goal을 기반으로 하는 user model을 정의할 수 있습니다. \n",
    "\n",
    "또한, `Agenda` policy는 dst 모델까지 함께 포함하고 있기 때문에 `user_dst = None` 이 됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load from https://convlab.blob.core.windows.net/convlab-2/milu_multiwoz_all_context.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0810 03:57:00.125682 139710458230592 archival.py:173] loading archive file /home/donghoon/.convlab2/cache/adeae6e5151198de3b1634fcb630a0243cd2e098a75703260c4489c6e92a5f51.4fff038b1c36dbb489d93575f604cf72276b651ac6b370b5a48f8f67df2ed971\n",
      "I0810 03:57:00.126493 139710458230592 archival.py:182] extracting archive file /home/donghoon/.convlab2/cache/adeae6e5151198de3b1634fcb630a0243cd2e098a75703260c4489c6e92a5f51.4fff038b1c36dbb489d93575f604cf72276b651ac6b370b5a48f8f67df2ed971 to temp dir /tmp/tmposxwr65b\n",
      "I0810 03:57:00.222218 139710458230592 registrable.py:73] instantiating registered subclass milu of <class 'allennlp.models.model.Model'>\n",
      "I0810 03:57:00.223016 139710458230592 params.py:265] type = default\n",
      "I0810 03:57:00.223470 139710458230592 registrable.py:73] instantiating registered subclass default of <class 'allennlp.data.vocabulary.Vocabulary'>\n",
      "I0810 03:57:00.223851 139710458230592 vocabulary.py:306] Loading token dictionary from /tmp/tmposxwr65b/vocabulary.\n",
      "I0810 03:57:00.237730 139710458230592 from_params.py:340] instantiating class <class 'allennlp.models.model.Model'> from params {'attention': {'matrix_dim': 400, 'type': 'bilinear', 'vector_dim': 400}, 'attention_for_intent': False, 'attention_for_tag': False, 'context_for_intent': True, 'context_for_tag': False, 'dropout': 0.3, 'encoder': {'bidirectional': True, 'dropout': 0.5, 'hidden_size': 200, 'input_size': 178, 'num_layers': 1, 'type': 'lstm'}, 'include_start_end_transitions': False, 'intent_encoder': {'bidirectional': True, 'dropout': 0.5, 'hidden_size': 200, 'input_size': 400, 'num_layers': 1, 'type': 'lstm'}, 'label_encoding': 'BIO', 'regularizer': [['scalar_parameters', {'alpha': 0.1, 'type': 'l2'}]], 'text_field_embedder': {'token_embedders': {'token_characters': {'embedding': {'embedding_dim': 16}, 'encoder': {'conv_layer_activation': 'relu', 'embedding_dim': 16, 'ngram_filter_sizes': [3], 'num_filters': 128, 'type': 'cnn'}, 'type': 'character_encoding'}, 'tokens': {'embedding_dim': 50, 'trainable': True, 'type': 'embedding'}}}, 'type': 'milu'} and extras {'vocab'}\n",
      "I0810 03:57:00.238294 139710458230592 params.py:265] model.type = milu\n",
      "I0810 03:57:00.239003 139710458230592 from_params.py:340] instantiating class <class 'convlab2.nlu.milu.model.MILU'> from params {'attention': {'matrix_dim': 400, 'type': 'bilinear', 'vector_dim': 400}, 'attention_for_intent': False, 'attention_for_tag': False, 'context_for_intent': True, 'context_for_tag': False, 'dropout': 0.3, 'encoder': {'bidirectional': True, 'dropout': 0.5, 'hidden_size': 200, 'input_size': 178, 'num_layers': 1, 'type': 'lstm'}, 'include_start_end_transitions': False, 'intent_encoder': {'bidirectional': True, 'dropout': 0.5, 'hidden_size': 200, 'input_size': 400, 'num_layers': 1, 'type': 'lstm'}, 'label_encoding': 'BIO', 'regularizer': [['scalar_parameters', {'alpha': 0.1, 'type': 'l2'}]], 'text_field_embedder': {'token_embedders': {'token_characters': {'embedding': {'embedding_dim': 16}, 'encoder': {'conv_layer_activation': 'relu', 'embedding_dim': 16, 'ngram_filter_sizes': [3], 'num_filters': 128, 'type': 'cnn'}, 'type': 'character_encoding'}, 'tokens': {'embedding_dim': 50, 'trainable': True, 'type': 'embedding'}}}} and extras {'vocab'}\n",
      "I0810 03:57:00.239865 139710458230592 from_params.py:340] instantiating class <class 'allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder'> from params {'token_embedders': {'token_characters': {'embedding': {'embedding_dim': 16}, 'encoder': {'conv_layer_activation': 'relu', 'embedding_dim': 16, 'ngram_filter_sizes': [3], 'num_filters': 128, 'type': 'cnn'}, 'type': 'character_encoding'}, 'tokens': {'embedding_dim': 50, 'trainable': True, 'type': 'embedding'}}} and extras {'vocab'}\n",
      "I0810 03:57:00.240364 139710458230592 params.py:265] model.text_field_embedder.type = basic\n",
      "I0810 03:57:00.240867 139710458230592 params.py:265] model.text_field_embedder.embedder_to_indexer_map = None\n",
      "I0810 03:57:00.241277 139710458230592 params.py:265] model.text_field_embedder.allow_unmatched_keys = False\n",
      "I0810 03:57:00.241703 139710458230592 from_params.py:340] instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'embedding': {'embedding_dim': 16}, 'encoder': {'conv_layer_activation': 'relu', 'embedding_dim': 16, 'ngram_filter_sizes': [3], 'num_filters': 128, 'type': 'cnn'}, 'type': 'character_encoding'} and extras {'vocab'}\n",
      "I0810 03:57:00.242109 139710458230592 params.py:265] model.text_field_embedder.token_embedders.token_characters.type = character_encoding\n",
      "I0810 03:57:00.242582 139710458230592 params.py:265] model.text_field_embedder.token_embedders.token_characters.embedding.num_embeddings = None\n",
      "I0810 03:57:00.242967 139710458230592 params.py:265] model.text_field_embedder.token_embedders.token_characters.embedding.vocab_namespace = token_characters\n",
      "I0810 03:57:00.243371 139710458230592 params.py:265] model.text_field_embedder.token_embedders.token_characters.embedding.embedding_dim = 16\n",
      "I0810 03:57:00.243732 139710458230592 params.py:265] model.text_field_embedder.token_embedders.token_characters.embedding.pretrained_file = None\n",
      "I0810 03:57:00.244093 139710458230592 params.py:265] model.text_field_embedder.token_embedders.token_characters.embedding.projection_dim = None\n",
      "I0810 03:57:00.244467 139710458230592 params.py:265] model.text_field_embedder.token_embedders.token_characters.embedding.trainable = True\n",
      "I0810 03:57:00.244863 139710458230592 params.py:265] model.text_field_embedder.token_embedders.token_characters.embedding.padding_index = None\n",
      "I0810 03:57:00.245331 139710458230592 params.py:265] model.text_field_embedder.token_embedders.token_characters.embedding.max_norm = None\n",
      "I0810 03:57:00.245724 139710458230592 params.py:265] model.text_field_embedder.token_embedders.token_characters.embedding.norm_type = 2.0\n",
      "I0810 03:57:00.246107 139710458230592 params.py:265] model.text_field_embedder.token_embedders.token_characters.embedding.scale_grad_by_freq = False\n",
      "I0810 03:57:00.246472 139710458230592 params.py:265] model.text_field_embedder.token_embedders.token_characters.embedding.sparse = False\n",
      "I0810 03:57:00.249134 139710458230592 from_params.py:340] instantiating class <class 'allennlp.modules.seq2vec_encoders.seq2vec_encoder.Seq2VecEncoder'> from params {'conv_layer_activation': 'relu', 'embedding_dim': 16, 'ngram_filter_sizes': [3], 'num_filters': 128, 'type': 'cnn'} and extras set()\n",
      "I0810 03:57:00.249527 139710458230592 params.py:265] model.text_field_embedder.token_embedders.token_characters.encoder.type = cnn\n",
      "I0810 03:57:00.249979 139710458230592 from_params.py:340] instantiating class <class 'allennlp.modules.seq2vec_encoders.cnn_encoder.CnnEncoder'> from params {'conv_layer_activation': 'relu', 'embedding_dim': 16, 'ngram_filter_sizes': [3], 'num_filters': 128} and extras set()\n",
      "I0810 03:57:00.250412 139710458230592 params.py:265] model.text_field_embedder.token_embedders.token_characters.encoder.embedding_dim = 16\n",
      "I0810 03:57:00.250970 139710458230592 params.py:265] model.text_field_embedder.token_embedders.token_characters.encoder.num_filters = 128\n",
      "I0810 03:57:00.251370 139710458230592 params.py:265] model.text_field_embedder.token_embedders.token_characters.encoder.ngram_filter_sizes = [3]\n",
      "I0810 03:57:00.251755 139710458230592 params.py:265] model.text_field_embedder.token_embedders.token_characters.encoder.conv_layer_activation = relu\n",
      "I0810 03:57:00.252153 139710458230592 registrable.py:73] instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
      "I0810 03:57:00.252549 139710458230592 params.py:265] model.text_field_embedder.token_embedders.token_characters.encoder.output_dim = None\n",
      "I0810 03:57:00.254091 139710458230592 params.py:265] model.text_field_embedder.token_embedders.token_characters.dropout = 0.0\n",
      "I0810 03:57:00.254545 139710458230592 from_params.py:340] instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'embedding_dim': 50, 'trainable': True, 'type': 'embedding'} and extras {'vocab'}\n",
      "I0810 03:57:00.254941 139710458230592 params.py:265] model.text_field_embedder.token_embedders.tokens.type = embedding\n",
      "I0810 03:57:00.255547 139710458230592 params.py:265] model.text_field_embedder.token_embedders.tokens.num_embeddings = None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0810 03:57:00.255993 139710458230592 params.py:265] model.text_field_embedder.token_embedders.tokens.vocab_namespace = tokens\n",
      "I0810 03:57:00.256966 139710458230592 params.py:265] model.text_field_embedder.token_embedders.tokens.embedding_dim = 50\n",
      "I0810 03:57:00.257428 139710458230592 params.py:265] model.text_field_embedder.token_embedders.tokens.pretrained_file = None\n",
      "I0810 03:57:00.257823 139710458230592 params.py:265] model.text_field_embedder.token_embedders.tokens.projection_dim = None\n",
      "I0810 03:57:00.258263 139710458230592 params.py:265] model.text_field_embedder.token_embedders.tokens.trainable = True\n",
      "I0810 03:57:00.258663 139710458230592 params.py:265] model.text_field_embedder.token_embedders.tokens.padding_index = None\n",
      "I0810 03:57:00.259059 139710458230592 params.py:265] model.text_field_embedder.token_embedders.tokens.max_norm = None\n",
      "I0810 03:57:00.259510 139710458230592 params.py:265] model.text_field_embedder.token_embedders.tokens.norm_type = 2.0\n",
      "I0810 03:57:00.259939 139710458230592 params.py:265] model.text_field_embedder.token_embedders.tokens.scale_grad_by_freq = False\n",
      "I0810 03:57:00.261401 139710458230592 params.py:265] model.text_field_embedder.token_embedders.tokens.sparse = False\n",
      "I0810 03:57:00.267467 139710458230592 from_params.py:340] instantiating class <class 'allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder'> from params {'bidirectional': True, 'dropout': 0.5, 'hidden_size': 200, 'input_size': 178, 'num_layers': 1, 'type': 'lstm'} and extras {'vocab'}\n",
      "I0810 03:57:00.267909 139710458230592 params.py:265] model.encoder.type = lstm\n",
      "I0810 03:57:00.268415 139710458230592 params.py:265] model.encoder.batch_first = True\n",
      "I0810 03:57:00.269209 139710458230592 params.py:265] model.encoder.stateful = False\n",
      "I0810 03:57:00.269579 139710458230592 params.py:394] Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "I0810 03:57:00.269940 139710458230592 params.py:397] CURRENTLY DEFINED PARAMETERS: \n",
      "I0810 03:57:00.270283 139710458230592 params.py:392] model.encoder.bidirectional = True\n",
      "I0810 03:57:00.270627 139710458230592 params.py:392] model.encoder.dropout = 0.5\n",
      "I0810 03:57:00.271021 139710458230592 params.py:392] model.encoder.hidden_size = 200\n",
      "I0810 03:57:00.271365 139710458230592 params.py:392] model.encoder.input_size = 178\n",
      "I0810 03:57:00.271737 139710458230592 params.py:392] model.encoder.num_layers = 1\n",
      "I0810 03:57:00.272136 139710458230592 params.py:392] model.encoder.batch_first = True\n",
      "I0810 03:57:00.277724 139710458230592 from_params.py:340] instantiating class <class 'allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder'> from params {'bidirectional': True, 'dropout': 0.5, 'hidden_size': 200, 'input_size': 400, 'num_layers': 1, 'type': 'lstm'} and extras {'vocab'}\n",
      "I0810 03:57:00.278226 139710458230592 params.py:265] model.intent_encoder.type = lstm\n",
      "I0810 03:57:00.278736 139710458230592 params.py:265] model.intent_encoder.batch_first = True\n",
      "I0810 03:57:00.279473 139710458230592 params.py:265] model.intent_encoder.stateful = False\n",
      "I0810 03:57:00.279913 139710458230592 params.py:394] Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "I0810 03:57:00.280589 139710458230592 params.py:397] CURRENTLY DEFINED PARAMETERS: \n",
      "I0810 03:57:00.281010 139710458230592 params.py:392] model.intent_encoder.bidirectional = True\n",
      "I0810 03:57:00.281716 139710458230592 params.py:392] model.intent_encoder.dropout = 0.5\n",
      "I0810 03:57:00.282137 139710458230592 params.py:392] model.intent_encoder.hidden_size = 200\n",
      "I0810 03:57:00.282808 139710458230592 params.py:392] model.intent_encoder.input_size = 400\n",
      "I0810 03:57:00.283240 139710458230592 params.py:392] model.intent_encoder.num_layers = 1\n",
      "I0810 03:57:00.283940 139710458230592 params.py:392] model.intent_encoder.batch_first = True\n",
      "I0810 03:57:00.290516 139710458230592 from_params.py:340] instantiating class <class 'allennlp.modules.attention.attention.Attention'> from params {'matrix_dim': 400, 'type': 'bilinear', 'vector_dim': 400} and extras {'vocab'}\n",
      "I0810 03:57:00.291331 139710458230592 params.py:265] model.attention.type = bilinear\n",
      "I0810 03:57:00.291997 139710458230592 from_params.py:340] instantiating class <class 'allennlp.modules.attention.bilinear_attention.BilinearAttention'> from params {'matrix_dim': 400, 'vector_dim': 400} and extras {'vocab'}\n",
      "I0810 03:57:00.292680 139710458230592 params.py:265] model.attention.vector_dim = 400\n",
      "I0810 03:57:00.293344 139710458230592 params.py:265] model.attention.matrix_dim = 400\n",
      "I0810 03:57:00.293808 139710458230592 params.py:265] model.attention.normalize = True\n",
      "I0810 03:57:00.294913 139710458230592 registrable.py:73] instantiating registered subclass linear of <class 'allennlp.nn.activations.Activation'>\n",
      "I0810 03:57:00.296280 139710458230592 params.py:265] model.context_for_intent = True\n",
      "I0810 03:57:00.296655 139710458230592 params.py:265] model.context_for_tag = False\n",
      "I0810 03:57:00.297080 139710458230592 params.py:265] model.attention_for_intent = False\n",
      "I0810 03:57:00.297481 139710458230592 params.py:265] model.attention_for_tag = False\n",
      "I0810 03:57:00.297869 139710458230592 params.py:265] model.sequence_label_namespace = labels\n",
      "I0810 03:57:00.298256 139710458230592 params.py:265] model.intent_label_namespace = intent_labels\n",
      "I0810 03:57:00.298661 139710458230592 params.py:265] model.label_encoding = BIO\n",
      "I0810 03:57:00.299063 139710458230592 params.py:265] model.include_start_end_transitions = False\n",
      "I0810 03:57:00.299453 139710458230592 params.py:265] model.crf_decoding = False\n",
      "I0810 03:57:00.299832 139710458230592 params.py:265] model.constrain_crf_decoding = None\n",
      "I0810 03:57:00.300211 139710458230592 params.py:265] model.focal_loss_gamma = None\n",
      "I0810 03:57:00.300590 139710458230592 params.py:265] model.nongeneral_intent_weight = 5.0\n",
      "I0810 03:57:00.300984 139710458230592 params.py:265] model.num_train_examples = None\n",
      "I0810 03:57:00.301400 139710458230592 params.py:265] model.calculate_span_f1 = None\n",
      "I0810 03:57:00.301798 139710458230592 params.py:265] model.dropout = 0.3\n",
      "I0810 03:57:00.302168 139710458230592 params.py:265] model.verbose_metrics = False\n",
      "I0810 03:57:00.302686 139710458230592 params.py:265] model.regularizer.0.1.type = l2\n",
      "I0810 03:57:00.303072 139710458230592 registrable.py:73] instantiating registered subclass l2 of <class 'allennlp.nn.regularizers.regularizer.Regularizer'>\n",
      "I0810 03:57:00.383509 139710458230592 initializers.py:307] Initializing parameters\n",
      "I0810 03:57:00.384152 139710458230592 initializers.py:323] Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "I0810 03:57:00.384604 139710458230592 initializers.py:328]    attention._bias\n",
      "I0810 03:57:00.385174 139710458230592 initializers.py:328]    attention._weight_matrix\n",
      "I0810 03:57:00.385671 139710458230592 initializers.py:328]    encoder._module.bias_hh_l0\n",
      "I0810 03:57:00.386161 139710458230592 initializers.py:328]    encoder._module.bias_hh_l0_reverse\n",
      "I0810 03:57:00.386642 139710458230592 initializers.py:328]    encoder._module.bias_ih_l0\n",
      "I0810 03:57:00.387118 139710458230592 initializers.py:328]    encoder._module.bias_ih_l0_reverse\n",
      "I0810 03:57:00.387605 139710458230592 initializers.py:328]    encoder._module.weight_hh_l0\n",
      "I0810 03:57:00.389290 139710458230592 initializers.py:328]    encoder._module.weight_hh_l0_reverse\n",
      "I0810 03:57:00.389801 139710458230592 initializers.py:328]    encoder._module.weight_ih_l0\n",
      "I0810 03:57:00.390305 139710458230592 initializers.py:328]    encoder._module.weight_ih_l0_reverse\n",
      "I0810 03:57:00.390792 139710458230592 initializers.py:328]    intent_encoder._module.bias_hh_l0\n",
      "I0810 03:57:00.391268 139710458230592 initializers.py:328]    intent_encoder._module.bias_hh_l0_reverse\n",
      "I0810 03:57:00.391691 139710458230592 initializers.py:328]    intent_encoder._module.bias_ih_l0\n",
      "I0810 03:57:00.392156 139710458230592 initializers.py:328]    intent_encoder._module.bias_ih_l0_reverse\n",
      "I0810 03:57:00.392570 139710458230592 initializers.py:328]    intent_encoder._module.weight_hh_l0\n",
      "I0810 03:57:00.393005 139710458230592 initializers.py:328]    intent_encoder._module.weight_hh_l0_reverse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0810 03:57:00.393446 139710458230592 initializers.py:328]    intent_encoder._module.weight_ih_l0\n",
      "I0810 03:57:00.395200 139710458230592 initializers.py:328]    intent_encoder._module.weight_ih_l0_reverse\n",
      "I0810 03:57:00.395625 139710458230592 initializers.py:328]    intent_projection_layer.bias\n",
      "I0810 03:57:00.396020 139710458230592 initializers.py:328]    intent_projection_layer.weight\n",
      "I0810 03:57:00.396461 139710458230592 initializers.py:328]    tag_projection_layer._module.bias\n",
      "I0810 03:57:00.396895 139710458230592 initializers.py:328]    tag_projection_layer._module.weight\n",
      "I0810 03:57:00.397929 139710458230592 initializers.py:328]    text_field_embedder.token_embedder_token_characters._embedding._module.weight\n",
      "I0810 03:57:00.398365 139710458230592 initializers.py:328]    text_field_embedder.token_embedder_token_characters._encoder._module.conv_layer_0.bias\n",
      "I0810 03:57:00.398777 139710458230592 initializers.py:328]    text_field_embedder.token_embedder_token_characters._encoder._module.conv_layer_0.weight\n",
      "I0810 03:57:00.399225 139710458230592 initializers.py:328]    text_field_embedder.token_embedder_tokens.weight\n",
      "I0810 03:57:00.416685 139710458230592 from_params.py:340] instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'context_size': 3, 'token_indexers': {'token_characters': {'min_padding_length': 3, 'type': 'characters'}, 'tokens': {'lowercase_tokens': True, 'type': 'single_id'}}, 'type': 'milu'} and extras set()\n",
      "I0810 03:57:00.417300 139710458230592 params.py:265] dataset_reader.type = milu\n",
      "I0810 03:57:00.417873 139710458230592 from_params.py:340] instantiating class <class 'convlab2.nlu.milu.dataset_reader.MILUDatasetReader'> from params {'context_size': 3, 'token_indexers': {'token_characters': {'min_padding_length': 3, 'type': 'characters'}, 'tokens': {'lowercase_tokens': True, 'type': 'single_id'}}} and extras set()\n",
      "I0810 03:57:00.418677 139710458230592 params.py:265] dataset_reader.context_size = 3\n",
      "I0810 03:57:00.419127 139710458230592 params.py:265] dataset_reader.agent = None\n",
      "I0810 03:57:00.419601 139710458230592 params.py:265] dataset_reader.random_context_size = True\n",
      "I0810 03:57:00.420052 139710458230592 params.py:265] dataset_reader.token_delimiter = None\n",
      "I0810 03:57:00.421116 139710458230592 from_params.py:340] instantiating class allennlp.data.token_indexers.token_indexer.TokenIndexer from params {'min_padding_length': 3, 'type': 'characters'} and extras set()\n",
      "I0810 03:57:00.421591 139710458230592 params.py:265] dataset_reader.token_indexers.token_characters.type = characters\n",
      "I0810 03:57:00.422140 139710458230592 from_params.py:340] instantiating class allennlp.data.token_indexers.token_characters_indexer.TokenCharactersIndexer from params {'min_padding_length': 3} and extras set()\n",
      "I0810 03:57:00.422612 139710458230592 params.py:265] dataset_reader.token_indexers.token_characters.namespace = token_characters\n",
      "I0810 03:57:00.423031 139710458230592 params.py:265] dataset_reader.token_indexers.token_characters.start_tokens = None\n",
      "I0810 03:57:00.423430 139710458230592 params.py:265] dataset_reader.token_indexers.token_characters.end_tokens = None\n",
      "I0810 03:57:00.423850 139710458230592 params.py:265] dataset_reader.token_indexers.token_characters.min_padding_length = 3\n",
      "I0810 03:57:00.424234 139710458230592 params.py:265] dataset_reader.token_indexers.token_characters.token_min_padding_length = 0\n",
      "I0810 03:57:00.424695 139710458230592 from_params.py:340] instantiating class allennlp.data.token_indexers.token_indexer.TokenIndexer from params {'lowercase_tokens': True, 'type': 'single_id'} and extras set()\n",
      "I0810 03:57:00.425119 139710458230592 params.py:265] dataset_reader.token_indexers.tokens.type = single_id\n",
      "I0810 03:57:00.425826 139710458230592 from_params.py:340] instantiating class allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer from params {'lowercase_tokens': True} and extras set()\n",
      "I0810 03:57:00.426450 139710458230592 params.py:265] dataset_reader.token_indexers.tokens.namespace = tokens\n",
      "I0810 03:57:00.427031 139710458230592 params.py:265] dataset_reader.token_indexers.tokens.lowercase_tokens = True\n",
      "I0810 03:57:00.427604 139710458230592 params.py:265] dataset_reader.token_indexers.tokens.start_tokens = None\n",
      "I0810 03:57:00.428170 139710458230592 params.py:265] dataset_reader.token_indexers.tokens.end_tokens = None\n",
      "I0810 03:57:00.428764 139710458230592 params.py:265] dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
      "I0810 03:57:00.429357 139710458230592 params.py:265] dataset_reader.lazy = False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading goal model is done\n"
     ]
    }
   ],
   "source": [
    "# MILU\n",
    "user_nlu = MILU()\n",
    "# not use dst\n",
    "user_dst = None\n",
    "# rule policy\n",
    "user_policy = RulePolicy(character='usr')   # UserPolicyAgendaMultiWoz()\n",
    "# template NLG\n",
    "user_nlg = TemplateNLG(is_user=True)\n",
    "# user_nlg = SCLSTM(is_user=True)\n",
    "# assemble\n",
    "user_agent = PipelineAgent(user_nlu, user_dst, user_policy, user_nlg, name='user')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.3 user simulator 와 system model 간 대화를 수행해봅시다.\n",
    "\n",
    "지금까지, 우리는 user simulator와 system model을 정의 했습니다.\n",
    "\n",
    "`MultiWozEvaluator` 클래스는 성능을 평가하기 위해 사용됩니다. (user의 goal을 정의해줍니다.)\n",
    "\n",
    "`BiSession` 클래스는 user simulator와 system model의 대화 및 평가를 도와줍니다.\n",
    "\n",
    "`next_turn` 함수는 한 턴의 대화를 수행합니다.\n",
    "\n",
    "### 평가 지표\n",
    "\n",
    "success rate : 예약 성사 + recall == 1, 즉 유저의 조건에 부합하는 예약을 해내고, 물어보는 모든 정보에 대해 알맞은 갑ㅅ을 출력\n",
    "\n",
    "book rate : 예약 성사율 (= 예약 성공 수 / 예약 정답 수) \n",
    "\n",
    "Inform precision : (TP) / (TP + FP), precision이 낮다는 것은 요청한 slot 외 불필요한 정보를 많이 알려주는 것으로 해석할 수 있음.\n",
    "\n",
    "Inform recall : (TP) / (TP + FN), recall이 낮다는 것은 요청한 slot에 대해 대답하지 못한 것으로 해석 할 수 있음.\n",
    "\n",
    "Inform F1 : Precision & Recall에 대한 조화 평균\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init goal:\n",
      "{'attraction': {'info': {'area': 'centre', 'type': 'museum'},\n",
      "                'reqt': {'address': '?', 'phone': '?'}},\n",
      " 'train': {'info': {'arriveBy': '08:30',\n",
      "                    'day': 'monday',\n",
      "                    'departure': 'cambridge',\n",
      "                    'destination': 'peterborough'},\n",
      "           'reqt': {'price': '?', 'trainID': '?'}}}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "user: I also need a train. I need some information on a train going to peterborough . I would like to leave on monday. I 'd like to arrive by 08:30 .\n",
      "sys: Where are you departing from ?\n",
      "\n",
      "user: I ' m also looking for a train from cambridge .\n",
      "sys: Would you like me to book the 08:30 train ? I can book your tickets for monday . Woudl you like me to book a train to peterborough for you ? Woudl you like me to book a train from cambridge for you ?\n",
      "\n",
      "user: I need to book a train to peterborough .\n",
      "sys: I can get you tickets for an arrival time at 08:30 , that is the soonest , is that okay ? Would you like to take the train on monday ? Woudl you like me to book a train to peterborough for you ? Woudl you like me to book a train from cambridge for you ?\n",
      "\n",
      "user: Give me the train id , please . Can you give me the price ?\n",
      "sys: The cost of the one way journey is 16.50 pounds . The train ID is TR3284 .\n",
      "\n",
      "user: I want a train leaving on monday .\n",
      "sys: I can get you tickets for an arrival time at 08:30 , that is the soonest , is that okay ? I can book you on monday. Woudl you like me to book a train to peterborough for you ? Woudl you like me to book a train from cambridge for you ?\n",
      "\n",
      "user: Can you help me plan a trip to see a particular attraction ? Do you have any museum attractions. Are there anything fun to do in city centre ?\n",
      "sys: I recommend the fitzwilliam museum , it 's got a lot of great features to watch !. We have 11 such location .\n",
      "\n",
      "user: Can you give me the phone number ? Could I get the address for it ?\n",
      "sys: The phone number is 01223314960 . It 's located at 98 king street .\n",
      "\n",
      "user: Actually , I ' m all set . Thank you ! Bye !.\n",
      "sys: Thank you and goodbye .\n",
      "\n",
      "task success: 1\n",
      "book rate: None\n",
      "inform precision/recall/f1: (1.0, 1.0, 1.0)\n",
      "--------------------------------------------------\n",
      "final goal:\n",
      "{'attraction': {'info': {'area': 'centre', 'type': 'museum'},\n",
      "                'reqt': {'address': '98 king street', 'phone': '01223314960'}},\n",
      " 'train': {'info': {'arriveBy': '08:30',\n",
      "                    'day': 'monday',\n",
      "                    'departure': 'cambridge',\n",
      "                    'destination': 'peterborough'},\n",
      "           'reqt': {'price': '16.50 pounds', 'trainID': 'TR3284'}}}\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "def set_seed(r_seed):\n",
    "    random.seed(r_seed)\n",
    "    np.random.seed(r_seed)\n",
    "    torch.manual_seed(r_seed)\n",
    "\n",
    "evaluator = MultiWozEvaluator()\n",
    "sess = BiSession(sys_agent=sys_agent, user_agent=user_agent, kb_query=None, evaluator=evaluator)\n",
    "\n",
    "set_seed(20200804)\n",
    "\n",
    "sys_response = ''\n",
    "sess.init_session()\n",
    "print('init goal:')\n",
    "pprint(sess.evaluator.goal)\n",
    "print('-'*100)\n",
    "for i in range(20):\n",
    "    sys_response, user_response, session_over, reward = sess.next_turn(sys_response)\n",
    "    print('user:', user_response)\n",
    "    print('sys:', sys_response)\n",
    "    print()\n",
    "    if session_over is True:\n",
    "        break\n",
    "print('task success:', sess.evaluator.task_success())\n",
    "print('book rate:', sess.evaluator.book_rate())\n",
    "print('inform precision/recall/f1:', sess.evaluator.inform_F1())\n",
    "print('-'*50)\n",
    "print('final goal:')\n",
    "pprint(sess.evaluator.goal)\n",
    "\n",
    "print('='*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3. ConvLab에서 제공하는 모듈들로 모델들을 구성 및 진단하고, 평가하자\n",
    "\n",
    "## Step 3.0. ConvLab 에서 지원하는 모델들을 load 합니다.\n",
    "\n",
    "이용가능한 model들:\n",
    "\n",
    "- NLU: BERTNLU, MILU, SVMNLU\n",
    "- DST: RuleDST\n",
    "- Word-DST: SUMBT, TRADE (set `sys_nlu` to `None`)\n",
    "- Policy: RulePolicy, Imitation, REINFORCE, PPO, GDPL\n",
    "- Word-Policy: MDRG, HDSA, LaRL (set `sys_nlg` to `None`)\n",
    "- NLG: Template, SCLSTM\n",
    "- End2End: Sequicity, DAMD, RNN_rollout (directly used as `sys_agent`)\n",
    "- Simulator policy: Agenda, VHUS (for `user_policy`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from:  https://convlab.blob.core.windows.net/convlab-2/mdrg_model.zip\n",
      "Load from https://convlab.blob.core.windows.net/convlab-2/mdrg_model.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0810 03:57:21.985568 139710458230592 allennlp_file_utils.py:284] https://convlab.blob.core.windows.net/convlab-2/mdrg_model.zip not found in cache, downloading to /tmp/tmp7lpjyuwq\n",
      "100%|██████████| 21577107/21577107 [00:07<00:00, 2887981.54B/s]\n",
      "I0810 03:57:30.093079 139710458230592 allennlp_file_utils.py:297] copying /tmp/tmp7lpjyuwq to cache at /home/donghoon/.convlab2/cache/b0bc758ff68dc79ef5287ddd38b6267f8784df273b2e6f7e496a1e9031c65ca5.ea9a4a5a9034b22be1093ea89deb230956f14487e2c2441b9ee59cef0fc252a2\n",
      "I0810 03:57:30.110977 139710458230592 allennlp_file_utils.py:301] creating metadata file for /home/donghoon/.convlab2/cache/b0bc758ff68dc79ef5287ddd38b6267f8784df273b2e6f7e496a1e9031c65ca5.ea9a4a5a9034b22be1093ea89deb230956f14487e2c2441b9ee59cef0fc252a2\n",
      "I0810 03:57:30.111685 139710458230592 allennlp_file_utils.py:307] removing temp file /tmp/tmp7lpjyuwq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting...\n",
      "Downloading from:  https://convlab.blob.core.windows.net/convlab-2/mdrg_data.zip\n",
      "Load from https://convlab.blob.core.windows.net/convlab-2/mdrg_data.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0810 03:57:30.809415 139710458230592 allennlp_file_utils.py:284] https://convlab.blob.core.windows.net/convlab-2/mdrg_data.zip not found in cache, downloading to /tmp/tmpqjwx40jc\n",
      "100%|██████████| 47104409/47104409 [00:18<00:00, 2574017.90B/s]\n",
      "I0810 03:57:49.786106 139710458230592 allennlp_file_utils.py:297] copying /tmp/tmpqjwx40jc to cache at /home/donghoon/.convlab2/cache/00a406587d87174b74198f14cae25cd2054923a471c59233f27ec80caef23686.da1518d0f3a98f95e2be9aee8474275aa3e182c5b9faccf16e9deac38752afce\n",
      "I0810 03:57:49.836905 139710458230592 allennlp_file_utils.py:301] creating metadata file for /home/donghoon/.convlab2/cache/00a406587d87174b74198f14cae25cd2054923a471c59233f27ec80caef23686.da1518d0f3a98f95e2be9aee8474275aa3e182c5b9faccf16e9deac38752afce\n",
      "I0810 03:57:49.837951 139710458230592 allennlp_file_utils.py:307] removing temp file /tmp/tmpqjwx40jc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting...\n",
      "Downloading from:  https://convlab.blob.core.windows.net/convlab-2/mdrg_db.zip\n",
      "Load from https://convlab.blob.core.windows.net/convlab-2/mdrg_db.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0810 03:57:53.406880 139710458230592 allennlp_file_utils.py:284] https://convlab.blob.core.windows.net/convlab-2/mdrg_db.zip not found in cache, downloading to /tmp/tmpksyu804s\n",
      "100%|██████████| 183081/183081 [00:00<00:00, 470357.32B/s]\n",
      "I0810 03:57:54.337714 139710458230592 allennlp_file_utils.py:297] copying /tmp/tmpksyu804s to cache at /home/donghoon/.convlab2/cache/a9766cc757fb79e7ac5266715dd065c687f884e5dc06840bba4d3b07307eb95b.b7bac7303e20c54957b367fa386215aaa595d5df9fb04341554b2067d458679c\n",
      "I0810 03:57:54.340874 139710458230592 allennlp_file_utils.py:301] creating metadata file for /home/donghoon/.convlab2/cache/a9766cc757fb79e7ac5266715dd065c687f884e5dc06840bba4d3b07307eb95b.b7bac7303e20c54957b367fa386215aaa595d5df9fb04341554b2067d458679c\n",
      "I0810 03:57:54.341880 139710458230592 allennlp_file_utils.py:307] removing temp file /tmp/tmpksyu804s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/donghoon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# available NLU models\n",
    "from convlab2.nlu.svm.multiwoz import SVMNLU\n",
    "from convlab2.nlu.jointBERT.multiwoz import BERTNLU\n",
    "from convlab2.nlu.milu.multiwoz import MILU\n",
    "# available DST models\n",
    "from convlab2.dst.rule.multiwoz import RuleDST\n",
    "#from convlab2.dst.mdbt.multiwoz import MDBT\n",
    "from convlab2.dst.sumbt.multiwoz import SUMBT\n",
    "from convlab2.dst.trade.multiwoz import TRADE\n",
    "# available Policy models\n",
    "from convlab2.policy.rule.multiwoz import RulePolicy\n",
    "from convlab2.policy.ppo.multiwoz import PPOPolicy\n",
    "from convlab2.policy.pg.multiwoz import PGPolicy\n",
    "from convlab2.policy.mle.multiwoz import MLEPolicy\n",
    "from convlab2.policy.gdpl.multiwoz import GDPLPolicy\n",
    "#from convlab2.policy.vhus.multiwoz import UserPolicyVHUS\n",
    "from convlab2.policy.mdrg.multiwoz import MDRGWordPolicy\n",
    "from convlab2.policy.hdsa.multiwoz import HDSA\n",
    "from convlab2.policy.larl.multiwoz import LaRL\n",
    "# available NLG models\n",
    "from convlab2.nlg.template.multiwoz import TemplateNLG\n",
    "from convlab2.nlg.sclstm.multiwoz import SCLSTM\n",
    "# available E2E models\n",
    "from convlab2.e2e.sequicity.multiwoz import Sequicity\n",
    "from convlab2.e2e.damd.multiwoz import Damd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.1. ConvLab에서 지원하는 모델들을 가지고 나만의 대화시스템을 만들어 봅시다.\n",
    "\n",
    "Word-DST 모델들은 NLU와 DST가 합쳐진 모델을 의미합니다. 따라서, 별도의 NLU 모델 없이 사용할 수 있습니다.\n",
    "\n",
    "따라서, (1) NLU+RuleDST 또는 (2) Word-DST 로 조합이 가능합니다.\n",
    "\n",
    "[주의!] Word-DST 의 경우 sys_nlu = None 이어야 합니다.\n",
    "\n",
    "Word-Policy 모델들은 Dialogue Policy 와 NLG 가 합쳐진 모델을 의미합니다. 따라서 별도의 NLG 모델없이 사용할 수 있습니다.\n",
    "\n",
    "따라서, (1) Policy+NLG 또는 Word-Policy 로 조합이 가능합니다.\n",
    "\n",
    "[주의!] Word-Policy 의 경우 sys_nlg = None 이어야 합니다.\n",
    "\n",
    "`PipelineAgent` class를 이용해 Pipelined 대화 시스템을 만들 수 있습니다. 또는 End-to-End model를 사용할 수도 있습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load from https://convlab.blob.core.windows.net/convlab-2/milu_multiwoz_all_context.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0810 03:58:12.280826 139710458230592 archival.py:173] loading archive file /home/donghoon/.convlab2/cache/adeae6e5151198de3b1634fcb630a0243cd2e098a75703260c4489c6e92a5f51.4fff038b1c36dbb489d93575f604cf72276b651ac6b370b5a48f8f67df2ed971\n",
      "I0810 03:58:12.281775 139710458230592 archival.py:182] extracting archive file /home/donghoon/.convlab2/cache/adeae6e5151198de3b1634fcb630a0243cd2e098a75703260c4489c6e92a5f51.4fff038b1c36dbb489d93575f604cf72276b651ac6b370b5a48f8f67df2ed971 to temp dir /tmp/tmprlutkp_7\n",
      "I0810 03:58:12.391796 139710458230592 registrable.py:73] instantiating registered subclass milu of <class 'allennlp.models.model.Model'>\n",
      "I0810 03:58:12.392410 139710458230592 params.py:265] type = default\n",
      "I0810 03:58:12.392838 139710458230592 registrable.py:73] instantiating registered subclass default of <class 'allennlp.data.vocabulary.Vocabulary'>\n",
      "I0810 03:58:12.393239 139710458230592 vocabulary.py:306] Loading token dictionary from /tmp/tmprlutkp_7/vocabulary.\n",
      "I0810 03:58:12.407484 139710458230592 from_params.py:340] instantiating class <class 'allennlp.models.model.Model'> from params {'attention': {'matrix_dim': 400, 'type': 'bilinear', 'vector_dim': 400}, 'attention_for_intent': False, 'attention_for_tag': False, 'context_for_intent': True, 'context_for_tag': False, 'dropout': 0.3, 'encoder': {'bidirectional': True, 'dropout': 0.5, 'hidden_size': 200, 'input_size': 178, 'num_layers': 1, 'type': 'lstm'}, 'include_start_end_transitions': False, 'intent_encoder': {'bidirectional': True, 'dropout': 0.5, 'hidden_size': 200, 'input_size': 400, 'num_layers': 1, 'type': 'lstm'}, 'label_encoding': 'BIO', 'regularizer': [['scalar_parameters', {'alpha': 0.1, 'type': 'l2'}]], 'text_field_embedder': {'token_embedders': {'token_characters': {'embedding': {'embedding_dim': 16}, 'encoder': {'conv_layer_activation': 'relu', 'embedding_dim': 16, 'ngram_filter_sizes': [3], 'num_filters': 128, 'type': 'cnn'}, 'type': 'character_encoding'}, 'tokens': {'embedding_dim': 50, 'trainable': True, 'type': 'embedding'}}}, 'type': 'milu'} and extras {'vocab'}\n",
      "I0810 03:58:12.408069 139710458230592 params.py:265] model.type = milu\n",
      "I0810 03:58:12.408829 139710458230592 from_params.py:340] instantiating class <class 'convlab2.nlu.milu.model.MILU'> from params {'attention': {'matrix_dim': 400, 'type': 'bilinear', 'vector_dim': 400}, 'attention_for_intent': False, 'attention_for_tag': False, 'context_for_intent': True, 'context_for_tag': False, 'dropout': 0.3, 'encoder': {'bidirectional': True, 'dropout': 0.5, 'hidden_size': 200, 'input_size': 178, 'num_layers': 1, 'type': 'lstm'}, 'include_start_end_transitions': False, 'intent_encoder': {'bidirectional': True, 'dropout': 0.5, 'hidden_size': 200, 'input_size': 400, 'num_layers': 1, 'type': 'lstm'}, 'label_encoding': 'BIO', 'regularizer': [['scalar_parameters', {'alpha': 0.1, 'type': 'l2'}]], 'text_field_embedder': {'token_embedders': {'token_characters': {'embedding': {'embedding_dim': 16}, 'encoder': {'conv_layer_activation': 'relu', 'embedding_dim': 16, 'ngram_filter_sizes': [3], 'num_filters': 128, 'type': 'cnn'}, 'type': 'character_encoding'}, 'tokens': {'embedding_dim': 50, 'trainable': True, 'type': 'embedding'}}}} and extras {'vocab'}\n",
      "I0810 03:58:12.409820 139710458230592 from_params.py:340] instantiating class <class 'allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder'> from params {'token_embedders': {'token_characters': {'embedding': {'embedding_dim': 16}, 'encoder': {'conv_layer_activation': 'relu', 'embedding_dim': 16, 'ngram_filter_sizes': [3], 'num_filters': 128, 'type': 'cnn'}, 'type': 'character_encoding'}, 'tokens': {'embedding_dim': 50, 'trainable': True, 'type': 'embedding'}}} and extras {'vocab'}\n",
      "I0810 03:58:12.410427 139710458230592 params.py:265] model.text_field_embedder.type = basic\n",
      "I0810 03:58:12.411038 139710458230592 params.py:265] model.text_field_embedder.embedder_to_indexer_map = None\n",
      "I0810 03:58:12.411622 139710458230592 params.py:265] model.text_field_embedder.allow_unmatched_keys = False\n",
      "I0810 03:58:12.412177 139710458230592 from_params.py:340] instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'embedding': {'embedding_dim': 16}, 'encoder': {'conv_layer_activation': 'relu', 'embedding_dim': 16, 'ngram_filter_sizes': [3], 'num_filters': 128, 'type': 'cnn'}, 'type': 'character_encoding'} and extras {'vocab'}\n",
      "I0810 03:58:12.412760 139710458230592 params.py:265] model.text_field_embedder.token_embedders.token_characters.type = character_encoding\n",
      "I0810 03:58:12.413365 139710458230592 params.py:265] model.text_field_embedder.token_embedders.token_characters.embedding.num_embeddings = None\n",
      "I0810 03:58:12.413901 139710458230592 params.py:265] model.text_field_embedder.token_embedders.token_characters.embedding.vocab_namespace = token_characters\n",
      "I0810 03:58:12.414388 139710458230592 params.py:265] model.text_field_embedder.token_embedders.token_characters.embedding.embedding_dim = 16\n",
      "I0810 03:58:12.414919 139710458230592 params.py:265] model.text_field_embedder.token_embedders.token_characters.embedding.pretrained_file = None\n",
      "I0810 03:58:12.415402 139710458230592 params.py:265] model.text_field_embedder.token_embedders.token_characters.embedding.projection_dim = None\n",
      "I0810 03:58:12.415944 139710458230592 params.py:265] model.text_field_embedder.token_embedders.token_characters.embedding.trainable = True\n",
      "I0810 03:58:12.416802 139710458230592 params.py:265] model.text_field_embedder.token_embedders.token_characters.embedding.padding_index = None\n",
      "I0810 03:58:12.417283 139710458230592 params.py:265] model.text_field_embedder.token_embedders.token_characters.embedding.max_norm = None\n",
      "I0810 03:58:12.417970 139710458230592 params.py:265] model.text_field_embedder.token_embedders.token_characters.embedding.norm_type = 2.0\n",
      "I0810 03:58:12.420092 139710458230592 params.py:265] model.text_field_embedder.token_embedders.token_characters.embedding.scale_grad_by_freq = False\n",
      "I0810 03:58:12.420590 139710458230592 params.py:265] model.text_field_embedder.token_embedders.token_characters.embedding.sparse = False\n",
      "I0810 03:58:12.421349 139710458230592 from_params.py:340] instantiating class <class 'allennlp.modules.seq2vec_encoders.seq2vec_encoder.Seq2VecEncoder'> from params {'conv_layer_activation': 'relu', 'embedding_dim': 16, 'ngram_filter_sizes': [3], 'num_filters': 128, 'type': 'cnn'} and extras set()\n",
      "I0810 03:58:12.422018 139710458230592 params.py:265] model.text_field_embedder.token_embedders.token_characters.encoder.type = cnn\n",
      "I0810 03:58:12.422541 139710458230592 from_params.py:340] instantiating class <class 'allennlp.modules.seq2vec_encoders.cnn_encoder.CnnEncoder'> from params {'conv_layer_activation': 'relu', 'embedding_dim': 16, 'ngram_filter_sizes': [3], 'num_filters': 128} and extras set()\n",
      "I0810 03:58:12.423181 139710458230592 params.py:265] model.text_field_embedder.token_embedders.token_characters.encoder.embedding_dim = 16\n",
      "I0810 03:58:12.423877 139710458230592 params.py:265] model.text_field_embedder.token_embedders.token_characters.encoder.num_filters = 128\n",
      "I0810 03:58:12.424410 139710458230592 params.py:265] model.text_field_embedder.token_embedders.token_characters.encoder.ngram_filter_sizes = [3]\n",
      "I0810 03:58:12.424859 139710458230592 params.py:265] model.text_field_embedder.token_embedders.token_characters.encoder.conv_layer_activation = relu\n",
      "I0810 03:58:12.425309 139710458230592 registrable.py:73] instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
      "I0810 03:58:12.425786 139710458230592 params.py:265] model.text_field_embedder.token_embedders.token_characters.encoder.output_dim = None\n",
      "I0810 03:58:12.429078 139710458230592 params.py:265] model.text_field_embedder.token_embedders.token_characters.dropout = 0.0\n",
      "I0810 03:58:12.429596 139710458230592 from_params.py:340] instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'embedding_dim': 50, 'trainable': True, 'type': 'embedding'} and extras {'vocab'}\n",
      "I0810 03:58:12.430051 139710458230592 params.py:265] model.text_field_embedder.token_embedders.tokens.type = embedding\n",
      "I0810 03:58:12.430880 139710458230592 params.py:265] model.text_field_embedder.token_embedders.tokens.num_embeddings = None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0810 03:58:12.431257 139710458230592 params.py:265] model.text_field_embedder.token_embedders.tokens.vocab_namespace = tokens\n",
      "I0810 03:58:12.431625 139710458230592 params.py:265] model.text_field_embedder.token_embedders.tokens.embedding_dim = 50\n",
      "I0810 03:58:12.431978 139710458230592 params.py:265] model.text_field_embedder.token_embedders.tokens.pretrained_file = None\n",
      "I0810 03:58:12.432303 139710458230592 params.py:265] model.text_field_embedder.token_embedders.tokens.projection_dim = None\n",
      "I0810 03:58:12.432699 139710458230592 params.py:265] model.text_field_embedder.token_embedders.tokens.trainable = True\n",
      "I0810 03:58:12.433143 139710458230592 params.py:265] model.text_field_embedder.token_embedders.tokens.padding_index = None\n",
      "I0810 03:58:12.433560 139710458230592 params.py:265] model.text_field_embedder.token_embedders.tokens.max_norm = None\n",
      "I0810 03:58:12.433982 139710458230592 params.py:265] model.text_field_embedder.token_embedders.tokens.norm_type = 2.0\n",
      "I0810 03:58:12.434392 139710458230592 params.py:265] model.text_field_embedder.token_embedders.tokens.scale_grad_by_freq = False\n",
      "I0810 03:58:12.434832 139710458230592 params.py:265] model.text_field_embedder.token_embedders.tokens.sparse = False\n",
      "I0810 03:58:12.442694 139710458230592 from_params.py:340] instantiating class <class 'allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder'> from params {'bidirectional': True, 'dropout': 0.5, 'hidden_size': 200, 'input_size': 178, 'num_layers': 1, 'type': 'lstm'} and extras {'vocab'}\n",
      "I0810 03:58:12.443349 139710458230592 params.py:265] model.encoder.type = lstm\n",
      "I0810 03:58:12.443949 139710458230592 params.py:265] model.encoder.batch_first = True\n",
      "I0810 03:58:12.444377 139710458230592 params.py:265] model.encoder.stateful = False\n",
      "I0810 03:58:12.444785 139710458230592 params.py:394] Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "I0810 03:58:12.445235 139710458230592 params.py:397] CURRENTLY DEFINED PARAMETERS: \n",
      "I0810 03:58:12.445605 139710458230592 params.py:392] model.encoder.bidirectional = True\n",
      "I0810 03:58:12.445949 139710458230592 params.py:392] model.encoder.dropout = 0.5\n",
      "I0810 03:58:12.446285 139710458230592 params.py:392] model.encoder.hidden_size = 200\n",
      "I0810 03:58:12.446625 139710458230592 params.py:392] model.encoder.input_size = 178\n",
      "I0810 03:58:12.446951 139710458230592 params.py:392] model.encoder.num_layers = 1\n",
      "I0810 03:58:12.447284 139710458230592 params.py:392] model.encoder.batch_first = True\n",
      "I0810 03:58:12.452681 139710458230592 from_params.py:340] instantiating class <class 'allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder'> from params {'bidirectional': True, 'dropout': 0.5, 'hidden_size': 200, 'input_size': 400, 'num_layers': 1, 'type': 'lstm'} and extras {'vocab'}\n",
      "I0810 03:58:12.453113 139710458230592 params.py:265] model.intent_encoder.type = lstm\n",
      "I0810 03:58:12.453560 139710458230592 params.py:265] model.intent_encoder.batch_first = True\n",
      "I0810 03:58:12.453936 139710458230592 params.py:265] model.intent_encoder.stateful = False\n",
      "I0810 03:58:12.454375 139710458230592 params.py:394] Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "I0810 03:58:12.454725 139710458230592 params.py:397] CURRENTLY DEFINED PARAMETERS: \n",
      "I0810 03:58:12.455238 139710458230592 params.py:392] model.intent_encoder.bidirectional = True\n",
      "I0810 03:58:12.455602 139710458230592 params.py:392] model.intent_encoder.dropout = 0.5\n",
      "I0810 03:58:12.455989 139710458230592 params.py:392] model.intent_encoder.hidden_size = 200\n",
      "I0810 03:58:12.456491 139710458230592 params.py:392] model.intent_encoder.input_size = 400\n",
      "I0810 03:58:12.456880 139710458230592 params.py:392] model.intent_encoder.num_layers = 1\n",
      "I0810 03:58:12.457270 139710458230592 params.py:392] model.intent_encoder.batch_first = True\n",
      "I0810 03:58:12.465265 139710458230592 from_params.py:340] instantiating class <class 'allennlp.modules.attention.attention.Attention'> from params {'matrix_dim': 400, 'type': 'bilinear', 'vector_dim': 400} and extras {'vocab'}\n",
      "I0810 03:58:12.465837 139710458230592 params.py:265] model.attention.type = bilinear\n",
      "I0810 03:58:12.466596 139710458230592 from_params.py:340] instantiating class <class 'allennlp.modules.attention.bilinear_attention.BilinearAttention'> from params {'matrix_dim': 400, 'vector_dim': 400} and extras {'vocab'}\n",
      "I0810 03:58:12.467253 139710458230592 params.py:265] model.attention.vector_dim = 400\n",
      "I0810 03:58:12.467832 139710458230592 params.py:265] model.attention.matrix_dim = 400\n",
      "I0810 03:58:12.468461 139710458230592 params.py:265] model.attention.normalize = True\n",
      "I0810 03:58:12.469247 139710458230592 registrable.py:73] instantiating registered subclass linear of <class 'allennlp.nn.activations.Activation'>\n",
      "I0810 03:58:12.470997 139710458230592 params.py:265] model.context_for_intent = True\n",
      "I0810 03:58:12.471446 139710458230592 params.py:265] model.context_for_tag = False\n",
      "I0810 03:58:12.471867 139710458230592 params.py:265] model.attention_for_intent = False\n",
      "I0810 03:58:12.472252 139710458230592 params.py:265] model.attention_for_tag = False\n",
      "I0810 03:58:12.472653 139710458230592 params.py:265] model.sequence_label_namespace = labels\n",
      "I0810 03:58:12.473151 139710458230592 params.py:265] model.intent_label_namespace = intent_labels\n",
      "I0810 03:58:12.473560 139710458230592 params.py:265] model.label_encoding = BIO\n",
      "I0810 03:58:12.473936 139710458230592 params.py:265] model.include_start_end_transitions = False\n",
      "I0810 03:58:12.474295 139710458230592 params.py:265] model.crf_decoding = False\n",
      "I0810 03:58:12.474734 139710458230592 params.py:265] model.constrain_crf_decoding = None\n",
      "I0810 03:58:12.475138 139710458230592 params.py:265] model.focal_loss_gamma = None\n",
      "I0810 03:58:12.475519 139710458230592 params.py:265] model.nongeneral_intent_weight = 5.0\n",
      "I0810 03:58:12.475901 139710458230592 params.py:265] model.num_train_examples = None\n",
      "I0810 03:58:12.476265 139710458230592 params.py:265] model.calculate_span_f1 = None\n",
      "I0810 03:58:12.476647 139710458230592 params.py:265] model.dropout = 0.3\n",
      "I0810 03:58:12.477038 139710458230592 params.py:265] model.verbose_metrics = False\n",
      "I0810 03:58:12.477557 139710458230592 params.py:265] model.regularizer.0.1.type = l2\n",
      "I0810 03:58:12.477939 139710458230592 registrable.py:73] instantiating registered subclass l2 of <class 'allennlp.nn.regularizers.regularizer.Regularizer'>\n",
      "I0810 03:58:12.567483 139710458230592 initializers.py:307] Initializing parameters\n",
      "I0810 03:58:12.568145 139710458230592 initializers.py:323] Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "I0810 03:58:12.568525 139710458230592 initializers.py:328]    attention._bias\n",
      "I0810 03:58:12.568965 139710458230592 initializers.py:328]    attention._weight_matrix\n",
      "I0810 03:58:12.569417 139710458230592 initializers.py:328]    encoder._module.bias_hh_l0\n",
      "I0810 03:58:12.570381 139710458230592 initializers.py:328]    encoder._module.bias_hh_l0_reverse\n",
      "I0810 03:58:12.570815 139710458230592 initializers.py:328]    encoder._module.bias_ih_l0\n",
      "I0810 03:58:12.571223 139710458230592 initializers.py:328]    encoder._module.bias_ih_l0_reverse\n",
      "I0810 03:58:12.571666 139710458230592 initializers.py:328]    encoder._module.weight_hh_l0\n",
      "I0810 03:58:12.572620 139710458230592 initializers.py:328]    encoder._module.weight_hh_l0_reverse\n",
      "I0810 03:58:12.573151 139710458230592 initializers.py:328]    encoder._module.weight_ih_l0\n",
      "I0810 03:58:12.573604 139710458230592 initializers.py:328]    encoder._module.weight_ih_l0_reverse\n",
      "I0810 03:58:12.574457 139710458230592 initializers.py:328]    intent_encoder._module.bias_hh_l0\n",
      "I0810 03:58:12.574964 139710458230592 initializers.py:328]    intent_encoder._module.bias_hh_l0_reverse\n",
      "I0810 03:58:12.575611 139710458230592 initializers.py:328]    intent_encoder._module.bias_ih_l0\n",
      "I0810 03:58:12.575983 139710458230592 initializers.py:328]    intent_encoder._module.bias_ih_l0_reverse\n",
      "I0810 03:58:12.576349 139710458230592 initializers.py:328]    intent_encoder._module.weight_hh_l0\n",
      "I0810 03:58:12.576716 139710458230592 initializers.py:328]    intent_encoder._module.weight_hh_l0_reverse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0810 03:58:12.577107 139710458230592 initializers.py:328]    intent_encoder._module.weight_ih_l0\n",
      "I0810 03:58:12.577471 139710458230592 initializers.py:328]    intent_encoder._module.weight_ih_l0_reverse\n",
      "I0810 03:58:12.577841 139710458230592 initializers.py:328]    intent_projection_layer.bias\n",
      "I0810 03:58:12.578907 139710458230592 initializers.py:328]    intent_projection_layer.weight\n",
      "I0810 03:58:12.579430 139710458230592 initializers.py:328]    tag_projection_layer._module.bias\n",
      "I0810 03:58:12.579805 139710458230592 initializers.py:328]    tag_projection_layer._module.weight\n",
      "I0810 03:58:12.580208 139710458230592 initializers.py:328]    text_field_embedder.token_embedder_token_characters._embedding._module.weight\n",
      "I0810 03:58:12.580600 139710458230592 initializers.py:328]    text_field_embedder.token_embedder_token_characters._encoder._module.conv_layer_0.bias\n",
      "I0810 03:58:12.580976 139710458230592 initializers.py:328]    text_field_embedder.token_embedder_token_characters._encoder._module.conv_layer_0.weight\n",
      "I0810 03:58:12.581382 139710458230592 initializers.py:328]    text_field_embedder.token_embedder_tokens.weight\n",
      "I0810 03:58:12.602731 139710458230592 from_params.py:340] instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'context_size': 3, 'token_indexers': {'token_characters': {'min_padding_length': 3, 'type': 'characters'}, 'tokens': {'lowercase_tokens': True, 'type': 'single_id'}}, 'type': 'milu'} and extras set()\n",
      "I0810 03:58:12.603305 139710458230592 params.py:265] dataset_reader.type = milu\n",
      "I0810 03:58:12.603761 139710458230592 from_params.py:340] instantiating class <class 'convlab2.nlu.milu.dataset_reader.MILUDatasetReader'> from params {'context_size': 3, 'token_indexers': {'token_characters': {'min_padding_length': 3, 'type': 'characters'}, 'tokens': {'lowercase_tokens': True, 'type': 'single_id'}}} and extras set()\n",
      "I0810 03:58:12.604187 139710458230592 params.py:265] dataset_reader.context_size = 3\n",
      "I0810 03:58:12.604614 139710458230592 params.py:265] dataset_reader.agent = None\n",
      "I0810 03:58:12.605038 139710458230592 params.py:265] dataset_reader.random_context_size = True\n",
      "I0810 03:58:12.605467 139710458230592 params.py:265] dataset_reader.token_delimiter = None\n",
      "I0810 03:58:12.605954 139710458230592 from_params.py:340] instantiating class allennlp.data.token_indexers.token_indexer.TokenIndexer from params {'min_padding_length': 3, 'type': 'characters'} and extras set()\n",
      "I0810 03:58:12.606412 139710458230592 params.py:265] dataset_reader.token_indexers.token_characters.type = characters\n",
      "I0810 03:58:12.607145 139710458230592 from_params.py:340] instantiating class allennlp.data.token_indexers.token_characters_indexer.TokenCharactersIndexer from params {'min_padding_length': 3} and extras set()\n",
      "I0810 03:58:12.607698 139710458230592 params.py:265] dataset_reader.token_indexers.token_characters.namespace = token_characters\n",
      "I0810 03:58:12.608120 139710458230592 params.py:265] dataset_reader.token_indexers.token_characters.start_tokens = None\n",
      "I0810 03:58:12.608542 139710458230592 params.py:265] dataset_reader.token_indexers.token_characters.end_tokens = None\n",
      "I0810 03:58:12.608991 139710458230592 params.py:265] dataset_reader.token_indexers.token_characters.min_padding_length = 3\n",
      "I0810 03:58:12.609382 139710458230592 params.py:265] dataset_reader.token_indexers.token_characters.token_min_padding_length = 0\n",
      "I0810 03:58:12.609884 139710458230592 from_params.py:340] instantiating class allennlp.data.token_indexers.token_indexer.TokenIndexer from params {'lowercase_tokens': True, 'type': 'single_id'} and extras set()\n",
      "I0810 03:58:12.610291 139710458230592 params.py:265] dataset_reader.token_indexers.tokens.type = single_id\n",
      "I0810 03:58:12.610717 139710458230592 from_params.py:340] instantiating class allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer from params {'lowercase_tokens': True} and extras set()\n",
      "I0810 03:58:12.611133 139710458230592 params.py:265] dataset_reader.token_indexers.tokens.namespace = tokens\n",
      "I0810 03:58:12.611566 139710458230592 params.py:265] dataset_reader.token_indexers.tokens.lowercase_tokens = True\n",
      "I0810 03:58:12.612035 139710458230592 params.py:265] dataset_reader.token_indexers.tokens.start_tokens = None\n",
      "I0810 03:58:12.612472 139710458230592 params.py:265] dataset_reader.token_indexers.tokens.end_tokens = None\n",
      "I0810 03:58:12.612861 139710458230592 params.py:265] dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
      "I0810 03:58:12.613293 139710458230592 params.py:265] dataset_reader.lazy = False\n"
     ]
    }
   ],
   "source": [
    "# NLU+RuleDST:\n",
    "sys_nlu = MILU()\n",
    "# sys_nlu = SVMNLU()\n",
    "# sys_nlu = BERTNLU()\n",
    "sys_dst = RuleDST()\n",
    "\n",
    "# or Word-DST:\n",
    "# sys_nlu = None\n",
    "# sys_dst = SUMBT()\n",
    "# sys_dst = TRADE()\n",
    "#### (not working!) sys_dst = MDBT()\n",
    "\n",
    "# [Caution] In Word-DST case, sys_nlu must be \"None\"\n",
    "\n",
    "# Policy+NLG:\n",
    "sys_policy = RulePolicy()\n",
    "# sys_policy = PPOPolicy()\n",
    "# sys_policy = PGPolicy()\n",
    "# sys_policy = MLEPolicy()\n",
    "# sys_policy = GDPLPolicy()\n",
    "sys_nlg = TemplateNLG(is_user=False)\n",
    "#sys_nlg = SCLSTM(is_user=False)\n",
    "\n",
    "# or Word-Policy:\n",
    "# sys_policy = LaRL()\n",
    "# sys_policy = HDSA()\n",
    "# sys_policy = MDRGWordPolicy()\n",
    "# sys_nlg = None\n",
    "\n",
    "# [Caution] \"In Word-policy case, sys_nlg must be None\"\n",
    "\n",
    "sys_agent = PipelineAgent(sys_nlu, sys_dst, sys_policy, sys_nlg, 'sys')\n",
    "# sys_agent = Sequicity()\n",
    "# sys_agent = Damd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞에서 했던 방식대로, user simulator도 정의해줍니다.\n",
    "\n",
    "(ConvLab에서는 RulePolicy(character='usr')로 두었을 때, `Agenda` policy로 변환되며, 이는 user의 goal을 기반으로 하는 user model을 정의할 수 있습니다. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load train, size 8434\n",
      "load val, size 999\n",
      "load test, size 1000\n",
      "loaded train, size 113500\n",
      "loaded val, size 14730\n",
      "loaded test, size 14744\n",
      "dialog act num: 36\n",
      "sentence label num: 137\n",
      "tag num: 331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0810 03:58:33.830533 139710458230592 file_utils.py:362] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmpgoc816tq\n",
      "I0810 03:58:35.263275 139710458230592 file_utils.py:377] copying /tmp/tmpgoc816tq to cache at /home/donghoon/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0810 03:58:35.264796 139710458230592 file_utils.py:381] creating metadata file for /home/donghoon/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0810 03:58:35.265638 139710458230592 file_utils.py:390] removing temp file /tmp/tmpgoc816tq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intent num: 137\n",
      "tag num: 331\n",
      "Load from model_file param\n",
      "Load from https://convlab.blob.core.windows.net/convlab-2/bert_multiwoz_all_context.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0810 03:58:35.878661 139710458230592 allennlp_file_utils.py:284] https://convlab.blob.core.windows.net/convlab-2/bert_multiwoz_all_context.zip not found in cache, downloading to /tmp/tmptdc44s1z\n",
      "100%|██████████| 425713245/425713245 [02:20<00:00, 3037041.82B/s]\n",
      "I0810 04:00:56.721940 139710458230592 allennlp_file_utils.py:297] copying /tmp/tmptdc44s1z to cache at /home/donghoon/.convlab2/cache/fe2b28201c498bb510ae89111e6bb1710a013c23920583adafc3f2b140376b90.4fbb3a3c9025fd8bc79740ec9ac9f931fb88de0cece837a569fba203fa3df2a0\n",
      "I0810 04:00:57.098271 139710458230592 allennlp_file_utils.py:301] creating metadata file for /home/donghoon/.convlab2/cache/fe2b28201c498bb510ae89111e6bb1710a013c23920583adafc3f2b140376b90.4fbb3a3c9025fd8bc79740ec9ac9f931fb88de0cece837a569fba203fa3df2a0\n",
      "I0810 04:00:57.099168 139710458230592 allennlp_file_utils.py:307] removing temp file /tmp/tmptdc44s1z\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load from /home/donghoon/PycharmProjects/ConvLab-2/convlab2/nlu/jointBERT/multiwoz/output/all_context/pytorch_model.bin\n",
      "bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0810 04:01:01.167612 139710458230592 file_utils.py:362] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /tmp/tmp6v7dyesv\n",
      "I0810 04:01:01.987807 139710458230592 file_utils.py:377] copying /tmp/tmp6v7dyesv to cache at /home/donghoon/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "I0810 04:01:01.989084 139710458230592 file_utils.py:381] creating metadata file for /home/donghoon/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "I0810 04:01:01.989931 139710458230592 file_utils.py:390] removing temp file /tmp/tmp6v7dyesv\n",
      "I0810 04:01:02.828496 139710458230592 file_utils.py:362] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/tmp1j4m8yck\n",
      "I0810 04:02:10.082777 139710458230592 file_utils.py:377] copying /tmp/tmp1j4m8yck to cache at /home/donghoon/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "I0810 04:02:10.521830 139710458230592 file_utils.py:381] creating metadata file for /home/donghoon/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "I0810 04:02:10.522521 139710458230592 file_utils.py:390] removing temp file /tmp/tmp1j4m8yck\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTNLU loaded\n",
      "Loading goal model is done\n"
     ]
    }
   ],
   "source": [
    "user_nlu = BERTNLU()\n",
    "# user_nlu = MILU()\n",
    "# user_nlu = SVMNLU()\n",
    "user_dst = None\n",
    "user_policy = RulePolicy(character='usr')\n",
    "# user_policy = UserPolicyVHUS(load_from_zip=True)\n",
    "user_nlg = TemplateNLG(is_user=True)\n",
    "# user_nlg = SCLSTM(is_user=True)\n",
    "user_agent = PipelineAgent(user_nlu, user_dst, user_policy, user_nlg, name='user')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.2 분석 툴을 사용해 system model을 진단해봅시다.\n",
    "\n",
    "Convlab2에서는 분석 툴(analysis tool)을 제공하며, 이를 통해 정의한 system model의 성능 및 취약점을 분석하고 진단할 수 있습니다.\n",
    "\n",
    "뿐만 아니라 HTML report를 작성해주어, 조금 더 풍부한 통계 정보를 얻어낼 수 있습니다. (results/\\$model_name\\$ 를 참조합니다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dialogue:   5%|▌         | 1/20 [00:01<00:19,  1.04s/it]I0810 04:02:15.939608 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [47932720631] (slot: phone domain: taxi)\n",
      "I0810 04:02:15.985555 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [57253443034] (slot: phone domain: taxi)\n",
      "I0810 04:02:16.030598 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [31316650087] (slot: phone domain: taxi)\n",
      "I0810 04:02:16.074352 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [85878627496] (slot: phone domain: taxi)\n",
      "I0810 04:02:16.118682 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [07233561887] (slot: phone domain: taxi)\n",
      "I0810 04:02:16.164804 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [17343656291] (slot: phone domain: taxi)\n",
      "I0810 04:02:16.216439 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [65450290121] (slot: phone domain: taxi)\n",
      "I0810 04:02:16.217009 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [87288110405] (slot: phone domain: taxi)\n",
      "I0810 04:02:16.265322 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [32650524200] (slot: phone domain: taxi)\n",
      "I0810 04:02:16.310998 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [67804874195] (slot: phone domain: taxi)\n",
      "I0810 04:02:16.355179 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [25994645825] (slot: phone domain: taxi)\n",
      "I0810 04:02:16.399540 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [51707383217] (slot: phone domain: taxi)\n",
      "I0810 04:02:16.450651 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [65504920416] (slot: phone domain: taxi)\n",
      "I0810 04:02:16.497462 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [18407935217] (slot: phone domain: taxi)\n",
      "dialogue:  15%|█▌        | 3/20 [00:02<00:12,  1.31it/s]I0810 04:02:16.767997 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [66] (slot: price domain: train)\n",
      "dialogue:  25%|██▌       | 5/20 [00:02<00:09,  1.63it/s]I0810 04:02:17.744656 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [16599984273] (slot: phone domain: taxi)\n",
      "I0810 04:02:17.790711 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [73545115219] (slot: phone domain: taxi)\n",
      "I0810 04:02:17.834725 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [49658361436] (slot: phone domain: taxi)\n",
      "I0810 04:02:17.881051 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [84820041884] (slot: phone domain: taxi)\n",
      "I0810 04:02:17.926222 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [79220920205] (slot: phone domain: taxi)\n",
      "I0810 04:02:17.970896 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [08329098485] (slot: phone domain: taxi)\n",
      "I0810 04:02:18.015652 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [91405620737] (slot: phone domain: taxi)\n",
      "I0810 04:02:18.060623 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [09130565871] (slot: phone domain: taxi)\n",
      "I0810 04:02:18.106095 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [10180974450] (slot: phone domain: taxi)\n",
      "I0810 04:02:18.151990 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [90925216566] (slot: phone domain: taxi)\n",
      "dialogue:  30%|███       | 6/20 [00:03<00:10,  1.39it/s]I0810 04:02:18.439494 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [saturday] (slot: leaveAt domain: train)\n",
      "I0810 04:02:18.440078 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [NOOFFER] (slot: departure domain: train)\n",
      "I0810 04:02:18.440624 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [Dest] (slot: departure domain: train)\n",
      "dialogue:  50%|█████     | 10/20 [00:04<00:03,  2.63it/s]I0810 04:02:19.424084 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [in town] (slot: area domain: restaurant)\n",
      "dialogue:  85%|████████▌ | 17/20 [00:07<00:01,  2.83it/s]I0810 04:02:21.907889 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [85939446618] (slot: phone domain: taxi)\n",
      "I0810 04:02:21.965349 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [39936927982] (slot: phone domain: taxi)\n",
      "I0810 04:02:22.025796 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [84722864780] (slot: phone domain: taxi)\n",
      "I0810 04:02:22.083388 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [27871012383] (slot: phone domain: taxi)\n",
      "I0810 04:02:22.137961 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [29195858899] (slot: phone domain: taxi)\n",
      "I0810 04:02:22.188910 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [82127923600] (slot: phone domain: taxi)\n",
      "I0810 04:02:22.245882 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [61388749070] (slot: phone domain: taxi)\n",
      "I0810 04:02:22.299725 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [64125283932] (slot: phone domain: taxi)\n",
      "I0810 04:02:22.395962 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [45516280473] (slot: phone domain: taxi)\n",
      "I0810 04:02:22.454503 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [43442327423] (slot: phone domain: taxi)\n",
      "I0810 04:02:22.507152 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [25837492007] (slot: phone domain: taxi)\n",
      "I0810 04:02:22.552668 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [80253096866] (slot: phone domain: taxi)\n",
      "I0810 04:02:22.598145 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [99792634644] (slot: phone domain: taxi)\n",
      "I0810 04:02:22.642673 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [96522664134] (slot: phone domain: taxi)\n",
      "dialogue:  90%|█████████ | 18/20 [00:08<00:01,  1.72it/s]I0810 04:02:22.979046 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [96864220773] (slot: phone domain: taxi)\n",
      "I0810 04:02:23.023462 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [46903350176] (slot: phone domain: taxi)\n",
      "I0810 04:02:23.072684 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [64685146323] (slot: phone domain: taxi)\n",
      "I0810 04:02:23.118746 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [46109889972] (slot: phone domain: taxi)\n",
      "I0810 04:02:23.164275 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [65908813256] (slot: phone domain: taxi)\n",
      "I0810 04:02:23.208237 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [47403647789] (slot: phone domain: taxi)\n",
      "I0810 04:02:23.253727 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [21559125785] (slot: phone domain: taxi)\n",
      "I0810 04:02:23.300907 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [97273996269] (slot: phone domain: taxi)\n",
      "I0810 04:02:23.301520 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [04480428708] (slot: phone domain: taxi)\n",
      "I0810 04:02:23.349183 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [86794794823] (slot: phone domain: taxi)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0810 04:02:23.393176 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [73986571951] (slot: phone domain: taxi)\n",
      "I0810 04:02:23.439137 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [91595639954] (slot: phone domain: taxi)\n",
      "I0810 04:02:23.485215 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [42110591073] (slot: phone domain: taxi)\n",
      "I0810 04:02:23.532055 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [20734023996] (slot: phone domain: taxi)\n",
      "I0810 04:02:23.576474 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [26091778855] (slot: phone domain: taxi)\n",
      "I0810 04:02:23.621210 139710458230592 policy_agenda_multiwoz.py:242] Value not found in standard value set: [77777216401] (slot: phone domain: taxi)\n",
      "dialogue: 100%|██████████| 20/20 [00:09<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "complete number of dialogs/tot: 0.7\n",
      "success number of dialogs/tot: 0.65\n",
      "average precision: 0.7517857142857143\n",
      "average recall: 0.8025\n",
      "average f1: 0.7711996336996336\n",
      "average book rate: 0.9761904761904762\n",
      "average turn (succ): 14.615384615384615\n",
      "average turn (all): 18.9\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7,\n",
       " 0.65,\n",
       " 0.7517857142857143,\n",
       " 0.8025,\n",
       " 0.7711996336996336,\n",
       " 0.9761904761904762,\n",
       " 18.9)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from convlab2.util.analysis_tool.analyzer import Analyzer\n",
    "\n",
    "# if sys_nlu!=None, set use_nlu=True to collect more information\n",
    "analyzer = Analyzer(user_agent=user_agent, dataset='multiwoz')\n",
    "\n",
    "set_seed(20200131)\n",
    "analyzer.comprehensive_analyze(sys_agent=sys_agent, model_name='sys_agent', total_dialog=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.3 여러 개의 system model 간 성능을 비교해봅시다. \n",
    "\n",
    "서로 다른 3가지의 system model에 대한 결과를 아래에 채워봅시다. (vscode 기준, 더블 클릭하면 수정할 수 있습니다.)\n",
    "\n",
    "NLU       | DST       | Policy    | NLG       | Success rate | Book rate | Inform P | Inform R | Inform F1 | Turn(succ/all) |\n",
    "--------- | --------- | --------- | --------- | :----------: | :-------: | -------- | -------- | --------- | -------------- |\n",
    "Content   | Content   | Content   | Content   | Content      | Content   | Content  | Content  | Content   | Content        |\n",
    "Content   | Content   | Content   | Content   | Content      | Content   | Content  | Content  | Content   | Content        |\n",
    "Content   | Content   | Content   | Content   | Content      | Content   | Content  | Content  | Content   | Content        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(20200805)\n",
    "\n",
    "# define your own system agent2\n",
    "# sys_agent2 = PipelineAgent(...)\n",
    "\n",
    "# define your own system agent3\n",
    "# sys_agent3 = PipelineAgent(...)\n",
    "\n",
    "analyzer.compare_models(agent_list=[sys_agent, sys_agent, sys_agent], model_name=['sys_agent1', 'sys_agent2', 'sys_agent3'], total_dialog=100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional. End-to-end Neural Pipeline (ACL 2020) 모델을 사용해보자\n",
    "\n",
    "Paper : Donghoon Ham *, Jeong-Gwan Lee *, Youngsoo Jang, and Kee-Eung Kim. 2020. End-to-End Neural Pipeline for Goal-Oriented Dialogue \u000b",
    "System using GPT-2. ACL 2020\n",
    "\n",
    "![Model architecture](image/e2e_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우선, Convlab2에 있는 모델을 import 하고 multiwoz로 pretrained된 weight를 다운로드 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/multiwoz/dialog_act_slot.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e440fc5f40b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mconvlab2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me2e\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTransformer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msys_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/PycharmProjects/ConvLab-2/convlab2/e2e/Transformer/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, model_checkpoint, max_history, device, no_sample, max_length, min_length, seed, temperature, top_k, top_p, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m                            \u001b[0;34m'taxi'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'leaveat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'destination'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'departure'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'arriveby'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                            'police': []}\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mdia_act\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/multiwoz/dialog_act_slot.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdia_act\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdia_act_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/multiwoz/dialog_act_slot.txt'"
     ]
    }
   ],
   "source": [
    "from convlab2.e2e.Transformer import Transformer\n",
    "sys_agent = Transformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 사용했던 다른 e2e agent 와는 다르게, neural pipeline 모델은 dialogue state, system action (dialogue policy) 을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_agent.init_session()\n",
    "sys_agent.response(\"I want to find a moderate hotel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural pipeline 모델과 대화해보고, 성능을 평가해봅시다 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_agent.init_session()\n",
    "while True:\n",
    "    raw_text = input(\">>> \")\n",
    "    while not raw_text:\n",
    "        print('not empty')\n",
    "        raw_text = input(\">>> \")\n",
    "    if raw_text == 'r':\n",
    "        sys_agent.init_session()\n",
    "        continue\n",
    "    out_text = sys_agent.response(raw_text)\n",
    "    print('sys: ', out_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = Analyzer(user_agent=user_agent, dataset='multiwoz')\n",
    "set_seed(20200131)\n",
    "analyzer.comprehensive_analyze(sys_agent=sys_agent, model_name='sys_agent', total_dialog=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:convlab2] *",
   "language": "python",
   "name": "conda-env-convlab2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
