{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1596371649204",
   "display_name": "Python 3.7.7 64-bit ('0813_dialogue_system': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import zipfile\n",
    "import sys\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_zipped_json(filepath, filename):\n",
    "    archive = zipfile.ZipFile(filepath, 'r')\n",
    "    return json.load(archive.open(filename))\n",
    "\n",
    "\n",
    "def phrase_in_utt(phrase, utt):\n",
    "    phrase_low = phrase.lower()\n",
    "    utt_low = utt.lower()\n",
    "    return (' ' + phrase_low in utt_low) or utt_low.startswith(phrase_low)\n",
    "\n",
    "\n",
    "def phrase_idx_utt(phrase, utt):\n",
    "    phrase_low = phrase.lower()\n",
    "    utt_low = utt.lower()\n",
    "    if ' ' + phrase_low in utt_low or utt_low.startswith(phrase_low):\n",
    "        return get_idx(phrase_low, utt_low)\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_idx(phrase, utt):\n",
    "    char_index_begin = utt.index(phrase)\n",
    "    char_index_end = char_index_begin + len(phrase)\n",
    "    word_index_begin = len(utt[:char_index_begin].split())\n",
    "    word_index_end = len(utt[:char_index_end].split()) - 1\n",
    "    return word_index_begin, word_index_end\n",
    "\n",
    "\n",
    "def da2triples(dialog_act):\n",
    "    triples = []\n",
    "    for intent, svs in dialog_act.items():\n",
    "        for slot, value in svs:\n",
    "            triples.append([intent, slot, value])\n",
    "    return triples\n",
    "\n",
    "\n",
    "def das2tags(sen, das):\n",
    "    tokens = word_tokenize(sen)\n",
    "    new_sen = ' '.join(tokens)\n",
    "    new_das = {}\n",
    "    span_info = []\n",
    "    intents = []\n",
    "    for da, svs in das.items():\n",
    "        new_das.setdefault(da, [])\n",
    "        if da == 'inform':\n",
    "            for s, v in svs:\n",
    "                v = ' '.join(word_tokenize(v))\n",
    "                if v != 'dontcare' and phrase_in_utt(v, new_sen):\n",
    "                    word_index_begin, word_index_end = phrase_idx_utt(v, new_sen)\n",
    "                    span_info.append((da, s, v, word_index_begin, word_index_end))\n",
    "                else:\n",
    "                    intents.append(da + '+' + s + '*' + v)\n",
    "                new_das[da].append([s, v])\n",
    "        else:\n",
    "            for s, v in svs:\n",
    "                new_das[da].append([s, v])\n",
    "                intents.append(da + '+' + s + '*' + v)\n",
    "    tags = []\n",
    "    for i, _ in enumerate(tokens):\n",
    "        for span in span_info:\n",
    "            if i == span[3]:\n",
    "                tag = \"B-\" + span[0] + \"+\" + span[1]\n",
    "                tags.append(tag)\n",
    "                break\n",
    "            if span[3] < i <= span[4]:\n",
    "                tag = \"I-\" + span[0] + \"+\" + span[1]\n",
    "                tags.append(tag)\n",
    "                break\n",
    "        else:\n",
    "            tags.append(\"O\")\n",
    "\n",
    "    return tokens, tags, intents, da2triples(new_das)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_dir = os.path.abspath(os.curdir)\n",
    "data_dir = \"ConvLab-2/data/camrest\"\n",
    "processed_data_dir = os.path.join(cur_dir, 'data/all_data')\n",
    "if not os.path.exists(processed_data_dir):\n",
    "    os.makedirs(processed_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "load train, size 406\nload val, size 135\nload test, size 135\n"
    }
   ],
   "source": [
    "data_key = ['train', 'val', 'test']\n",
    "data = {}\n",
    "for key in data_key:\n",
    "    data[key] = read_zipped_json(os.path.join(data_dir, key + '.json.zip'), key + '.json')\n",
    "    print('load {}, size {}'.format(key, len(data[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "loaded train, size 3342\nloaded val, size 1076\nloaded test, size 1070\n"
    }
   ],
   "source": [
    "mode = 'all'\n",
    "processed_data = {}\n",
    "all_da = []\n",
    "all_intent = []\n",
    "all_tag = []\n",
    "context_size = 3\n",
    "for key in data_key:\n",
    "    processed_data[key] = []\n",
    "    for dialog in data[key]:\n",
    "        context = []\n",
    "        for turn in dialog['dial']:\n",
    "            if mode == 'usr' or mode == 'all':\n",
    "                tokens, tags, intents, new_das = das2tags(turn['usr']['transcript'], turn['usr']['dialog_act'])\n",
    "\n",
    "                processed_data[key].append([tokens, tags, intents, new_das, context[-context_size:]])\n",
    "\n",
    "                all_da += [da for da in turn['usr']['dialog_act']]\n",
    "                all_intent += intents\n",
    "                all_tag += tags\n",
    "\n",
    "            context.append(turn['usr']['transcript'])\n",
    "\n",
    "            if mode == 'sys' or mode == 'all':\n",
    "                tokens, tags, intents, new_das = das2tags(turn['sys']['sent'], turn['sys']['dialog_act'])\n",
    "\n",
    "                processed_data[key].append([tokens, tags, intents, new_das, context[-context_size:]])\n",
    "                all_da += [da for da in turn['sys']['dialog_act']]\n",
    "                all_intent += intents\n",
    "                all_tag += tags\n",
    "\n",
    "            context.append(turn['sys']['sent'])\n",
    "\n",
    "    print('loaded {}, size {}'.format(key, len(processed_data[key])))\n",
    "    json.dump(processed_data[key], \n",
    "              open(os.path.join(processed_data_dir, '{}_data.json'.format(key)), 'w'),\n",
    "              indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "dialog act num: 4437\nsentence label num: 2428\ntag num: 68055\n"
    }
   ],
   "source": [
    "print('dialog act num:', len(all_da))\n",
    "print('sentence label num:', len(all_intent))\n",
    "print('tag num:', len(all_tag))\n",
    "json.dump(all_da, open(os.path.join(processed_data_dir, 'all_act.json'), 'w'), indent=2)\n",
    "json.dump(all_intent, open(os.path.join(processed_data_dir, 'intent_vocab.json'), 'w'), indent=2)\n",
    "json.dump(all_tag, open(os.path.join(processed_data_dir, 'tag_vocab.json'), 'w'), indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. BERT Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "--------------------dataset:camrest--------------------\n"
    }
   ],
   "source": [
    "print('-' * 20 + 'dataset:camrest' + '-' * 20)\n",
    "from convlab2.nlu.jointBERT.camrest.postprocess import is_slot_da, calculateF1, recover_intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "intent num: 2428\ntag num: 68055\nmax sen bert len 50\n[(1, 4), (2, 19), (3, 17), (4, 68), (5, 125), (6, 43), (7, 45), (8, 62), (9, 73), (10, 41), (11, 53), (12, 44), (13, 54), (14, 42), (15, 43), (16, 37), (17, 38), (18, 29), (19, 27), (20, 26), (21, 18), (22, 26), (23, 26), (24, 21), (25, 21), (26, 13), (27, 6), (28, 8), (29, 7), (30, 6), (31, 13), (32, 4), (33, 1), (34, 4), (35, 2), (36, 1), (39, 4), (40, 2), (43, 2), (50, 1)]\nmax context bert len 108\n[(3, 135), (9, 3), (10, 3), (11, 3), (12, 7), (13, 11), (14, 8), (15, 12), (16, 10), (17, 12), (18, 15), (19, 11), (20, 16), (21, 4), (22, 10), (23, 9), (24, 10), (25, 14), (26, 13), (27, 15), (28, 8), (29, 10), (30, 15), (31, 22), (32, 18), (33, 17), (34, 12), (35, 23), (36, 17), (37, 21), (38, 17), (39, 29), (40, 19), (41, 18), (42, 25), (43, 28), (44, 25), (45, 23), (46, 18), (47, 11), (48, 19), (49, 27), (50, 20), (51, 21), (52, 26), (53, 22), (54, 12), (55, 16), (56, 15), (57, 13), (58, 15), (59, 9), (60, 6), (61, 11), (62, 15), (63, 14), (64, 11), (65, 7), (66, 8), (67, 12), (68, 5), (69, 5), (70, 5), (71, 7), (72, 3), (73, 4), (74, 4), (75, 6), (76, 3), (77, 2), (78, 2), (79, 4), (80, 6), (81, 1), (82, 2), (83, 4), (84, 2), (85, 3), (86, 2), (88, 2), (89, 1), (90, 1), (91, 1), (92, 1), (95, 1), (96, 1), (99, 1), (108, 1)]\nval set size: 1076\nmax sen bert len 47\n[(1, 6), (2, 34), (3, 28), (4, 57), (5, 114), (6, 58), (7, 59), (8, 62), (9, 44), (10, 43), (11, 48), (12, 41), (13, 42), (14, 45), (15, 40), (16, 31), (17, 53), (18, 42), (19, 29), (20, 27), (21, 25), (22, 20), (23, 14), (24, 19), (25, 15), (26, 17), (27, 12), (28, 6), (29, 4), (30, 4), (31, 5), (32, 7), (33, 1), (34, 4), (35, 2), (36, 6), (38, 1), (39, 2), (40, 1), (41, 1), (47, 1)]\nmax context bert len 110\n[(3, 135), (7, 1), (9, 2), (10, 1), (11, 1), (12, 10), (13, 10), (14, 8), (15, 11), (16, 12), (17, 12), (18, 15), (19, 10), (20, 7), (21, 18), (22, 13), (23, 13), (24, 8), (25, 10), (26, 9), (27, 9), (28, 7), (29, 13), (30, 12), (31, 15), (32, 11), (33, 16), (34, 18), (35, 13), (36, 22), (37, 10), (38, 22), (39, 27), (40, 25), (41, 27), (42, 37), (43, 19), (44, 25), (45, 23), (46, 19), (47, 23), (48, 20), (49, 23), (50, 14), (51, 28), (52, 11), (53, 20), (54, 31), (55, 10), (56, 21), (57, 16), (58, 12), (59, 20), (60, 18), (61, 12), (62, 11), (63, 8), (64, 7), (65, 4), (66, 4), (67, 7), (68, 10), (69, 3), (70, 6), (71, 9), (72, 4), (73, 6), (74, 4), (75, 3), (76, 2), (77, 5), (78, 3), (79, 2), (80, 2), (81, 5), (83, 1), (85, 1), (86, 1), (91, 2), (93, 1), (94, 1), (103, 2), (110, 1)]\ntest set size: 1070\n"
    }
   ],
   "source": [
    "data_dir = os.path.join(cur_dir, 'data/all_data')\n",
    "output_dir = os.path.join(cur_dir, 'outputs')\n",
    "log_dir = os.path.join(cur_dir, 'logs')\n",
    "\n",
    "max_len = 40\n",
    "pretrained_model_name = \"bert-base-uncased\"\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "intent_vocab = json.load(open(os.path.join(data_dir, 'intent_vocab.json')))\n",
    "tag_vocab = json.load(open(os.path.join(data_dir, 'tag_vocab.json')))\n",
    "dataloader = Dataloader(intent_vocab=intent_vocab, tag_vocab=tag_vocab,\n",
    "                        pretrained_weights=pretrained_model_name)\n",
    "\n",
    "print('intent num:', len(intent_vocab))\n",
    "print('tag num:', len(tag_vocab))\n",
    "for data_key in ['val', 'test']:\n",
    "    dataloader.load_data(json.load(open(os.path.join(data_dir, '{}_data.json'.format(data_key)))), data_key,\n",
    "                            cut_sen_len=max_len, use_bert_tokenizer=True)\n",
    "    print('{} set size: {}'.format(data_key, len(dataloader.data[data_key])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[['inform', 'food', 'brazilian'], ['inform', 'area', 'north']]"
     },
     "metadata": {},
     "execution_count": 97
    }
   ],
   "source": [
    "# word_seq_tensor, tag_seq_tensor, intent_tensor, word_mask_tensor, tag_mask_tensor, context_seq_tensor, context_mask_tensor\n",
    "dataloader.data['val'][0][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ]
}